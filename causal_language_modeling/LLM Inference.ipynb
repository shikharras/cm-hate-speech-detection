{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/envs/owl-botu/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "import torch\n",
    "#from torch.utils.data import Dataset\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, pipeline\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data & Create Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prompt Example**:\n",
    "\n",
    "You are an expert in hate speech detection. Offensive tweets are defined as tweets containing profane words, sarcastic remarks, insults, slanders or slurs. These can have a potentially harmful effect on a given target. Classify the following input tweet as Offensive or Non-Offensive.\n",
    "\n",
    "`###` Input: <tweet>\n",
    "\n",
    "`###` Response: Offensive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are an expert in hate speech detection. Offensive tweets are defined as tweets containing profane words, sarcastic remarks, insults, slanders or slurs. These can have a potentially harmful effect on a given target. Classify the following input tweet as Offensive or Non-Offensive.\"\n",
    "label_map = {1: \"Offensive\", 0: \"Non-Offensive\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_prompt(row, train=True):\n",
    "    # Data Format -- https://huggingface.co/datasets/vicgalle/alpaca-gpt4?row=0\n",
    "    prompt = system_prompt + \"\\n\\n ### Input: \" + row[\"tweet\"] + \"\\n\\n ### Response: \"\n",
    "    if train:\n",
    "         prompt = prompt + label_map[row[\"offense\"]] # Add label\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BloomForCausalLM(\n",
       "  (transformer): BloomModel(\n",
       "    (word_embeddings): Embedding(250880, 4096)\n",
       "    (word_embeddings_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "    (h): ModuleList(\n",
       "      (0-29): 30 x BloomBlock(\n",
       "        (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): BloomAttention(\n",
       "          (query_key_value): Linear(\n",
       "            in_features=4096, out_features=12288, bias=True\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=64, out_features=12288, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "          )\n",
       "          (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BloomMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (gelu_impl): BloomGelu()\n",
       "          (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=250880, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_checkpoint = \"results_bloomz/checkpoint-5109/\" # \"results/checkpoint-5109/\"  llama-test/\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(trained_checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(trained_checkpoint)\n",
    "model = model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"data/cm_hate_combined.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['You are an expert in hate speech detection. Offensive tweets are defined as tweets containing profane words, sarcastic remarks, insults, slanders or slurs. These can have a potentially harmful effect on a given target. Classify the following input tweet as Offensive or Non-Offensive.\\n\\n ### Input: @user @user @user @user @user Matlab sirf ladki ke character baat ithae tab bologe 0ar ladke ke upar wo bhi khud ke fd se karoge to chup rahoge.\\n\\n ### Response: ',\n",
       "       'You are an expert in hate speech detection. Offensive tweets are defined as tweets containing profane words, sarcastic remarks, insults, slanders or slurs. These can have a potentially harmful effect on a given target. Classify the following input tweet as Offensive or Non-Offensive.\\n\\n ### Input: Pehle main bahut loyal tha tab mujhse koi ladki nahi pat rahi thi phir ek din....\\n \\n\\n Phir kya abhi bhi koi nahi pat rahi\\n (Kyuki abhi bhi loyal hi hu)\\n\\n ### Response: '],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df[\"text\"] = test_df.apply(lambda row: prepare_prompt(row, train=False), axis=1)\n",
    "test_df[\"text\"].values[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_responses(df):\n",
    "    model.eval()\n",
    "    responses = []\n",
    "\n",
    "    for i in tqdm(range(len(df))):\n",
    "        inputs = tokenizer(df[\"text\"][i], padding=True, truncation=True, max_length=300, \n",
    "                           return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            generate_ids = model.generate(inputs.input_ids, max_length=300)\n",
    "\n",
    "        response = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, \n",
    "                                          clean_up_tokenization_spaces=False)[0]\n",
    "        responses.append(response)\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(responses):\n",
    "    labels = []\n",
    "    response_trimmed = 0\n",
    "    label_absent = 0\n",
    "\n",
    "    for response in responses:\n",
    "        splitted = response.split(\"### Response: \")\n",
    "        if len(splitted) == 1:\n",
    "            #print(response, \"\\n\")\n",
    "            response_trimmed += 1\n",
    "            label = 0 #-1\n",
    "            \n",
    "        else:\n",
    "            if \"Non-Offensive\" in splitted[1][:15]:\n",
    "                label = 0\n",
    "            elif \"Offensive\" in splitted[1][:15]:\n",
    "                label = 1\n",
    "            else:\n",
    "                label_absent += 1\n",
    "                label = 0 # Default majority class\n",
    "                \n",
    "        labels.append(label)\n",
    "\n",
    "    print(f\"{response_trimmed} responses trimmed due to max_length\")\n",
    "    print(f\"{label_absent} labels absent \\n\")\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(labels, df):\n",
    "    print(\"F1 score = \", f1_score(df['offense'].tolist(), labels))\n",
    "    print(classification_report(df['offense'].tolist(), labels, \n",
    "                                target_names=[\"Non-Offensive (0)\", \"Offensive (1)\"], digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 641/641 [02:50<00:00,  3.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 responses trimmed due to max_length\n",
      "0 labels absent \n",
      "\n",
      "F1 score =  0.593103448275862\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "Non-Offensive (0)     0.6853    0.6436    0.6638       362\n",
      "    Offensive (1)     0.5714    0.6165    0.5931       279\n",
      "\n",
      "         accuracy                         0.6318       641\n",
      "        macro avg     0.6284    0.6301    0.6285       641\n",
      "     weighted avg     0.6357    0.6318    0.6330       641\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_responses = inference_responses(test_df)\n",
    "test_labels = get_labels(test_responses)\n",
    "print_metrics(test_labels, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/predictions/bloomz-ft-completion_custom_data.pickle', 'wb') as f:\n",
    "    pickle.dump(test_labels, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "owl-botu",
   "language": "python",
   "name": "owl-botu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
