{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Px3XMA4ud5LH"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.metrics import f1_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "id": "P4ypoDbHd8rk",
    "outputId": "9d162b80-81ff-409a-978a-4f1ffcf1c406"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>aggression</th>\n",
       "      <th>offense</th>\n",
       "      <th>codemixed</th>\n",
       "      <th>tweet_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6587</td>\n",
       "      <td>169269</td>\n",
       "      <td>1.585690e+18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Let's get some zimbabwe players into ipl and i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6807</td>\n",
       "      <td>178238</td>\n",
       "      <td>1.555080e+18</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>@user What about millions of undertrials langu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9120</td>\n",
       "      <td>449</td>\n",
       "      <td>1.580290e+18</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>@user 😂 he has to pay for it .. he burnt gandh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5210</td>\n",
       "      <td>133816</td>\n",
       "      <td>1.583590e+18</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>@user ratio + mojitos outsold + only men drink...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4309</td>\n",
       "      <td>120099</td>\n",
       "      <td>1.540990e+18</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>@user In India, ‘right-wing’ BJP govt gave wom...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0      id      tweet_id  aggression  offense  codemixed  \\\n",
       "0        6587  169269  1.585690e+18           0        0          1   \n",
       "1        6807  178238  1.555080e+18           2        0          0   \n",
       "2        9120     449  1.580290e+18           2        1          1   \n",
       "3        5210  133816  1.583590e+18           0        1          0   \n",
       "4        4309  120099  1.540990e+18           0        1          0   \n",
       "\n",
       "                                          tweet_text  \n",
       "0  Let's get some zimbabwe players into ipl and i...  \n",
       "1  @user What about millions of undertrials langu...  \n",
       "2  @user 😂 he has to pay for it .. he burnt gandh...  \n",
       "3  @user ratio + mojitos outsold + only men drink...  \n",
       "4  @user In India, ‘right-wing’ BJP govt gave wom...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"data/splits/train.csv\")\n",
    "val = pd.read_csv(\"data/splits/val.csv\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6T9YaNb9l1qN"
   },
   "source": [
    "Exp Prompt:\n",
    "\n",
    "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request. ### Instruction: Evaluate this sentence for spelling and grammar mistakes ### Input: He finnished his meal and left the resturant ### Response: There are two spelling errors in the sentence. The corrected sentence should be: \"He finished his meal and left the restaurant.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fZ3hXb1HmtED"
   },
   "source": [
    "My Prompt:\n",
    "\n",
    "You are an expert in hate speech detection. Offensive tweets are defined as tweets containing profane words, sarcastic remarks, insults, slanders or slurs. These can have a potentially harmful effect on a given target. Classify the following input tweet as Offensive or Non-Offensive.\n",
    "\n",
    "`###` Input: <tweet>\n",
    "\n",
    "`###` Response: Offensive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "2beQZ5PXn-fD"
   },
   "outputs": [],
   "source": [
    "system_prompt = \"You are an expert in hate speech detection. Offensive tweets are defined as tweets containing profane words, sarcastic remarks, insults, slanders or slurs. These can have a potentially harmful effect on a given target. Classify the following input tweet as Offensive or Non-Offensive.\"\n",
    "label_map = {1: \"Offensive\", 0: \"Non-Offensive\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "A499b8gDeGF5"
   },
   "outputs": [],
   "source": [
    "def prepare_prompt(row):\n",
    "    # Data Format -- https://huggingface.co/datasets/vicgalle/alpaca-gpt4?row=0\n",
    "    prompt = system_prompt + \"\\n\\n### Input: \" + row[\"tweet_text\"] + \"\\n\\n### Response: \" + label_map[row[\"offense\"]]\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UiETRup4eP0i",
    "outputId": "e835ae2f-ab2e-4e2e-a52e-1e7b0253be28"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"You are an expert in hate speech detection. Offensive tweets are defined as tweets containing profane words, sarcastic remarks, insults, slanders or slurs. These can have a potentially harmful effect on a given target. Classify the following input tweet as Offensive or Non-Offensive.\\n\\n### Input: Let's get some zimbabwe players into ipl and invite them to play series against india in india #zimbabwe #PAKvsZIM #T20worldcup22\\n\\n### Response: Non-Offensive\",\n",
       "       'You are an expert in hate speech detection. Offensive tweets are defined as tweets containing profane words, sarcastic remarks, insults, slanders or slurs. These can have a potentially harmful effect on a given target. Classify the following input tweet as Offensive or Non-Offensive.\\n\\n### Input: @user What about millions of undertrials languishing in jails for years without hearings. Recently 121 inmates were released after in captive for five years without proof. Are they not normal human beings?\\n\\n### Response: Non-Offensive',\n",
       "       'You are an expert in hate speech detection. Offensive tweets are defined as tweets containing profane words, sarcastic remarks, insults, slanders or slurs. These can have a potentially harmful effect on a given target. Classify the following input tweet as Offensive or Non-Offensive.\\n\\n### Input: @user 😂 he has to pay for it .. he burnt gandhis face in billion times over in the name of this cruel #demonetisation ..\\n\\n### Response: Offensive'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"text\"] = train.apply(lambda row: prepare_prompt(row), axis=1)\n",
    "val[\"text\"] = val.apply(lambda row: prepare_prompt(row), axis=1)\n",
    "\n",
    "train[\"text\"].values[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "CnxI0PcEfNIN"
   },
   "outputs": [],
   "source": [
    "train[[\"id\", \"text\", \"offense\"]].to_csv(\"data/llama_test/train.csv\", index=False)\n",
    "val[[\"id\", \"text\", \"offense\"]].to_csv(\"data/llama_test/val.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "LEUowrmXdJbr"
   },
   "outputs": [],
   "source": [
    "# MODEL: TinyPixel/Llama-2-7B-bf16-sharded --- Model sharded into 14 smaller models ~ 1gb each\n",
    "#        abhishek/llama-2-7b-hf-small-shards --- 10 shards\n",
    "# Max Length can go upto 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FJkjXZlFr9Ge",
    "outputId": "c64eb228-ac13-4245-e478-b364a3fbeb50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: autotrain <command> [<args>] llm [-h] [--train] [--deploy]\r\n",
      "                                        [--inference] [--data_path DATA_PATH]\r\n",
      "                                        [--train_split TRAIN_SPLIT]\r\n",
      "                                        [--valid_split VALID_SPLIT]\r\n",
      "                                        [--text_column TEXT_COLUMN]\r\n",
      "                                        [--rejected_text_column REJECTED_TEXT_COLUMN]\r\n",
      "                                        [--prompt-text-column PROMPT_TEXT_COLUMN]\r\n",
      "                                        [--model MODEL]\r\n",
      "                                        [--model-ref MODEL_REF]\r\n",
      "                                        [--learning_rate LEARNING_RATE]\r\n",
      "                                        [--num_train_epochs NUM_TRAIN_EPOCHS]\r\n",
      "                                        [--train_batch_size TRAIN_BATCH_SIZE]\r\n",
      "                                        [--warmup_ratio WARMUP_RATIO]\r\n",
      "                                        [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]\r\n",
      "                                        [--optimizer OPTIMIZER]\r\n",
      "                                        [--scheduler SCHEDULER]\r\n",
      "                                        [--weight_decay WEIGHT_DECAY]\r\n",
      "                                        [--max_grad_norm MAX_GRAD_NORM]\r\n",
      "                                        [--seed SEED] [--add_eos_token]\r\n",
      "                                        [--block_size BLOCK_SIZE] [--use_peft]\r\n",
      "                                        [--lora_r LORA_R]\r\n",
      "                                        [--lora_alpha LORA_ALPHA]\r\n",
      "                                        [--lora_dropout LORA_DROPOUT]\r\n",
      "                                        [--logging_steps LOGGING_STEPS]\r\n",
      "                                        [--project_name PROJECT_NAME]\r\n",
      "                                        [--evaluation_strategy EVALUATION_STRATEGY]\r\n",
      "                                        [--save_total_limit SAVE_TOTAL_LIMIT]\r\n",
      "                                        [--save_strategy SAVE_STRATEGY]\r\n",
      "                                        [--auto_find_batch_size] [--fp16]\r\n",
      "                                        [--push_to_hub] [--use_int8]\r\n",
      "                                        [--model_max_length MODEL_MAX_LENGTH]\r\n",
      "                                        [--repo_id REPO_ID] [--use_int4]\r\n",
      "                                        [--trainer TRAINER]\r\n",
      "                                        [--target_modules TARGET_MODULES]\r\n",
      "                                        [--merge_adapter] [--token TOKEN]\r\n",
      "                                        [--backend BACKEND]\r\n",
      "                                        [--username USERNAME]\r\n",
      "                                        [--use_flash_attention_2] [--log LOG]\r\n",
      "                                        [--disable_gradient_checkpointing]\r\n",
      "                                        [--dpo-beta DPO_BETA]\r\n",
      "\r\n",
      "✨ Run AutoTrain LLM\r\n",
      "\r\n",
      "options:\r\n",
      "  -h, --help            show this help message and exit\r\n",
      "  --train               Train the model\r\n",
      "  --deploy              Deploy the model\r\n",
      "  --inference           Run inference\r\n",
      "  --data_path DATA_PATH, --data-path DATA_PATH\r\n",
      "                        Train dataset to use\r\n",
      "  --train_split TRAIN_SPLIT, --train-split TRAIN_SPLIT\r\n",
      "                        Test dataset split to use\r\n",
      "  --valid_split VALID_SPLIT, --valid-split VALID_SPLIT\r\n",
      "                        Validation dataset split to use\r\n",
      "  --text_column TEXT_COLUMN, --text-column TEXT_COLUMN\r\n",
      "                        Text column to use\r\n",
      "  --rejected_text_column REJECTED_TEXT_COLUMN, --rejected-text-column REJECTED_TEXT_COLUMN\r\n",
      "                        Rejected text column to use\r\n",
      "  --prompt-text-column PROMPT_TEXT_COLUMN, --prompt-text-column PROMPT_TEXT_COLUMN\r\n",
      "                        Prompt text column to use\r\n",
      "  --model MODEL         Model to use\r\n",
      "  --model-ref MODEL_REF\r\n",
      "                        Reference model to use for DPO when not using PEFT\r\n",
      "  --learning_rate LEARNING_RATE, --lr LEARNING_RATE, --learning-rate LEARNING_RATE\r\n",
      "                        Learning rate to use\r\n",
      "  --num_train_epochs NUM_TRAIN_EPOCHS, --epochs NUM_TRAIN_EPOCHS\r\n",
      "                        Number of training epochs to use\r\n",
      "  --train_batch_size TRAIN_BATCH_SIZE, --train-batch-size TRAIN_BATCH_SIZE, --batch-size TRAIN_BATCH_SIZE\r\n",
      "                        Training batch size to use\r\n",
      "  --warmup_ratio WARMUP_RATIO, --warmup-ratio WARMUP_RATIO\r\n",
      "                        Warmup proportion to use\r\n",
      "  --gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS, --gradient-accumulation-steps GRADIENT_ACCUMULATION_STEPS, --gradient-accumulation GRADIENT_ACCUMULATION_STEPS\r\n",
      "                        Gradient accumulation steps to use\r\n",
      "  --optimizer OPTIMIZER\r\n",
      "                        Optimizer to use\r\n",
      "  --scheduler SCHEDULER\r\n",
      "                        Scheduler to use\r\n",
      "  --weight_decay WEIGHT_DECAY, --weight-decay WEIGHT_DECAY\r\n",
      "                        Weight decay to use\r\n",
      "  --max_grad_norm MAX_GRAD_NORM, --max-grad-norm MAX_GRAD_NORM\r\n",
      "                        Max gradient norm to use\r\n",
      "  --seed SEED           Seed to use\r\n",
      "  --add_eos_token, --add-eos-token\r\n",
      "                        Add EOS token to use\r\n",
      "  --block_size BLOCK_SIZE, --block-size BLOCK_SIZE\r\n",
      "                        Block size to use\r\n",
      "  --use_peft, --use-peft\r\n",
      "                        Use PEFT to use\r\n",
      "  --lora_r LORA_R, --lora-r LORA_R\r\n",
      "                        Lora r to use\r\n",
      "  --lora_alpha LORA_ALPHA, --lora-alpha LORA_ALPHA\r\n",
      "                        Lora alpha to use\r\n",
      "  --lora_dropout LORA_DROPOUT, --lora-dropout LORA_DROPOUT\r\n",
      "                        Lora dropout to use\r\n",
      "  --logging_steps LOGGING_STEPS, --logging-steps LOGGING_STEPS\r\n",
      "                        Logging steps to use\r\n",
      "  --project_name PROJECT_NAME, --project-name PROJECT_NAME\r\n",
      "                        Output directory\r\n",
      "  --evaluation_strategy EVALUATION_STRATEGY, --evaluation-strategy EVALUATION_STRATEGY\r\n",
      "                        Evaluation strategy to use\r\n",
      "  --save_total_limit SAVE_TOTAL_LIMIT, --save-total-limit SAVE_TOTAL_LIMIT\r\n",
      "                        Save total limit to use\r\n",
      "  --save_strategy SAVE_STRATEGY, --save-strategy SAVE_STRATEGY\r\n",
      "                        Save strategy to use\r\n",
      "  --auto_find_batch_size, --auto-find-batch-size\r\n",
      "                        Auto find batch size True/False\r\n",
      "  --fp16                FP16 True/False\r\n",
      "  --push_to_hub, --push-to-hub\r\n",
      "                        Push to hub True/False. In case you want to push the\r\n",
      "                        trained model to huggingface hub\r\n",
      "  --use_int8, --use-int8\r\n",
      "                        Use int8 True/False\r\n",
      "  --model_max_length MODEL_MAX_LENGTH, --max-len MODEL_MAX_LENGTH, --max-length MODEL_MAX_LENGTH\r\n",
      "                        Model max length to use\r\n",
      "  --repo_id REPO_ID, --repo-id REPO_ID\r\n",
      "                        Repo id for hugging face hub. Format is\r\n",
      "                        username/repo_name\r\n",
      "  --use_int4, --use-int4\r\n",
      "                        Use int4 True/False\r\n",
      "  --trainer TRAINER     Trainer type to use\r\n",
      "  --target_modules TARGET_MODULES, --target-modules TARGET_MODULES\r\n",
      "                        Target modules to use\r\n",
      "  --merge_adapter, --merge-adapter\r\n",
      "                        Use this flag to merge PEFT adapter with the model\r\n",
      "  --token TOKEN         Hugingface token to use\r\n",
      "  --backend BACKEND     Backend to use: default or spaces. Spaces backend\r\n",
      "                        requires push_to_hub and repo_id\r\n",
      "  --username USERNAME   Huggingface username to use\r\n",
      "  --use_flash_attention_2, --use-flash-attention-2, --use-fa2\r\n",
      "                        Use flash attention 2\r\n",
      "  --log LOG             Use experiment tracking\r\n",
      "  --disable_gradient_checkpointing, --disable-gradient-checkpointing, --disable-gc\r\n",
      "                        Disable gradient checkpointing\r\n",
      "  --dpo-beta DPO_BETA, --dpo-beta DPO_BETA\r\n",
      "                        Beta for DPO trainer\r\n"
     ]
    }
   ],
   "source": [
    "!autotrain llm --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install flash-attn --no-build-isolation\n",
    "\n",
    "# FlashAttention-2 currently supports:\n",
    "# Ampere, Ada, or Hopper GPUs (e.g., A100, RTX 3090, RTX 4090, H100). \n",
    "# Support for Turing GPUs (T4, RTX 2080) is coming soon, please use FlashAttention 1.x for Turing GPUs for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lU3y2Pg-YQTU",
    "outputId": "49b9ab65-0e3f-4fc4-bb6e-8d4f1b5e49f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[1mINFO    Running LLM\u001b[0m\n",
      "> \u001b[1mINFO    Params: Namespace(version=False, train=True, deploy=False, inference=False, data_path='data/llama_test', train_split='train', valid_split='val', text_column='text', rejected_text_column='rejected', prompt_text_column='prompt', model='TinyPixel/Llama-2-7B-bf16-sharded', model_ref=None, learning_rate=3e-05, num_train_epochs=5, train_batch_size=4, warmup_ratio=0.1, gradient_accumulation_steps=1, optimizer='adamw_torch', scheduler='linear', weight_decay=0.0, max_grad_norm=1.0, seed=42, add_eos_token=False, block_size=-1, use_peft=True, lora_r=16, lora_alpha=32, lora_dropout=0.05, logging_steps=-1, project_name='llama-test', evaluation_strategy='epoch', save_total_limit=1, save_strategy='epoch', auto_find_batch_size=False, fp16=False, push_to_hub=False, use_int8=False, model_max_length=1024, repo_id=None, use_int4=True, trainer='default', target_modules=None, merge_adapter=False, token=None, backend='default', username=None, use_flash_attention_2=False, log='none', disable_gradient_checkpointing=False, dpo_beta=0.1, func=<function run_llm_command_factory at 0x15391bf18550>)\u001b[0m\n",
      "> \u001b[1mINFO    loading dataset from csv\u001b[0m\n",
      "> \u001b[1mINFO    loading dataset from csv\u001b[0m\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "Loading checkpoint shards: 100%|████████████████| 14/14 [00:26<00:00,  1.89s/it]\n",
      "> \u001b[1mINFO    Using block size 1024\u001b[0m\n",
      "Running tokenizer on train dataset: 100%|█| 6809/6809 [00:01<00:00, 3788.29 exam\n",
      "Running tokenizer on validation dataset: 100%|█| 852/852 [00:00<00:00, 4275.53 e\n",
      "Grouping texts in chunks of 1024 (num_proc=4): 100%|█| 6809/6809 [00:00<00:00, 7\n",
      "/ext3/miniconda3/envs/owl-botu/lib/python3.10/site-packages/datasets/table.py:1395: FutureWarning: promote has been superseded by mode='default'.\n",
      "  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\n",
      "/ext3/miniconda3/envs/owl-botu/lib/python3.10/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n",
      "Grouping texts in chunks of 1024 (num_proc=4): 100%|█| 852/852 [00:00<00:00, 328\n",
      "> \u001b[1mINFO    creating trainer\u001b[0m\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "  0%|                                                  | 0/1165 [00:00<?, ?it/s]/ext3/miniconda3/envs/owl-botu/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "{'loss': 1.6739, 'learning_rate': 1.2820512820512822e-06, 'epoch': 0.02}        \n",
      "{'loss': 1.6805, 'learning_rate': 2.5641025641025644e-06, 'epoch': 0.04}        \n",
      "{'loss': 1.699, 'learning_rate': 3.846153846153846e-06, 'epoch': 0.06}          \n",
      "{'loss': 1.6739, 'learning_rate': 5.128205128205129e-06, 'epoch': 0.09}         \n",
      "{'loss': 1.6919, 'learning_rate': 6.41025641025641e-06, 'epoch': 0.11}          \n",
      "{'loss': 1.7528, 'learning_rate': 7.692307692307692e-06, 'epoch': 0.13}         \n",
      "{'loss': 1.6775, 'learning_rate': 8.974358974358974e-06, 'epoch': 0.15}         \n",
      "{'loss': 1.6597, 'learning_rate': 1.0256410256410258e-05, 'epoch': 0.17}        \n",
      "{'loss': 1.6775, 'learning_rate': 1.153846153846154e-05, 'epoch': 0.19}         \n",
      "{'loss': 1.635, 'learning_rate': 1.282051282051282e-05, 'epoch': 0.21}          \n",
      "{'loss': 1.5929, 'learning_rate': 1.4102564102564104e-05, 'epoch': 0.24}        \n",
      "{'loss': 1.5553, 'learning_rate': 1.5384615384615384e-05, 'epoch': 0.26}        \n",
      "{'loss': 1.5551, 'learning_rate': 1.6666666666666667e-05, 'epoch': 0.28}        \n",
      "{'loss': 1.5125, 'learning_rate': 1.7948717948717948e-05, 'epoch': 0.3}         \n",
      "{'loss': 1.5248, 'learning_rate': 1.923076923076923e-05, 'epoch': 0.32}         \n",
      "{'loss': 1.5299, 'learning_rate': 2.0512820512820515e-05, 'epoch': 0.34}        \n",
      "{'loss': 1.3954, 'learning_rate': 2.1794871794871795e-05, 'epoch': 0.36}        \n",
      "{'loss': 1.4201, 'learning_rate': 2.307692307692308e-05, 'epoch': 0.39}         \n",
      "{'loss': 1.4843, 'learning_rate': 2.435897435897436e-05, 'epoch': 0.41}         \n",
      "{'loss': 1.36, 'learning_rate': 2.564102564102564e-05, 'epoch': 0.43}           \n",
      "{'loss': 1.3853, 'learning_rate': 2.6923076923076923e-05, 'epoch': 0.45}        \n",
      "{'loss': 1.373, 'learning_rate': 2.8205128205128207e-05, 'epoch': 0.47}         \n",
      "{'loss': 1.3437, 'learning_rate': 2.9487179487179487e-05, 'epoch': 0.49}        \n",
      "{'loss': 1.2967, 'learning_rate': 2.991412213740458e-05, 'epoch': 0.52}         \n",
      "{'loss': 1.2742, 'learning_rate': 2.9770992366412214e-05, 'epoch': 0.54}        \n",
      "{'loss': 1.2809, 'learning_rate': 2.962786259541985e-05, 'epoch': 0.56}         \n",
      "{'loss': 1.3098, 'learning_rate': 2.948473282442748e-05, 'epoch': 0.58}         \n",
      "{'loss': 1.309, 'learning_rate': 2.9341603053435116e-05, 'epoch': 0.6}          \n",
      "{'loss': 1.2524, 'learning_rate': 2.9198473282442748e-05, 'epoch': 0.62}        \n",
      "{'loss': 1.2501, 'learning_rate': 2.9055343511450383e-05, 'epoch': 0.64}        \n",
      "{'loss': 1.2003, 'learning_rate': 2.8912213740458018e-05, 'epoch': 0.67}        \n",
      "{'loss': 1.2415, 'learning_rate': 2.876908396946565e-05, 'epoch': 0.69}         \n",
      "{'loss': 1.2512, 'learning_rate': 2.8625954198473285e-05, 'epoch': 0.71}        \n",
      "{'loss': 1.226, 'learning_rate': 2.8482824427480917e-05, 'epoch': 0.73}         \n",
      "{'loss': 1.2207, 'learning_rate': 2.833969465648855e-05, 'epoch': 0.75}         \n",
      "{'loss': 1.2143, 'learning_rate': 2.8196564885496187e-05, 'epoch': 0.77}        \n",
      "{'loss': 1.2095, 'learning_rate': 2.8053435114503815e-05, 'epoch': 0.79}        \n",
      "{'loss': 1.1953, 'learning_rate': 2.791030534351145e-05, 'epoch': 0.82}         \n",
      "{'loss': 1.1793, 'learning_rate': 2.7767175572519082e-05, 'epoch': 0.84}        \n",
      "{'loss': 1.2584, 'learning_rate': 2.7624045801526717e-05, 'epoch': 0.86}        \n",
      "{'loss': 1.2198, 'learning_rate': 2.7480916030534352e-05, 'epoch': 0.88}        \n",
      "{'loss': 1.2039, 'learning_rate': 2.7337786259541984e-05, 'epoch': 0.9}         \n",
      "{'loss': 1.1953, 'learning_rate': 2.719465648854962e-05, 'epoch': 0.92}         \n",
      "{'loss': 1.1493, 'learning_rate': 2.705152671755725e-05, 'epoch': 0.94}         \n",
      "{'loss': 1.1113, 'learning_rate': 2.6908396946564886e-05, 'epoch': 0.97}        \n",
      "{'loss': 1.0949, 'learning_rate': 2.676526717557252e-05, 'epoch': 0.99}         \n",
      " 20%|███████▌                              | 233/1165 [20:33<1:22:38,  5.32s/it]\n",
      "  0%|                                                    | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "  7%|███                                         | 2/29 [00:01<00:22,  1.19it/s]\u001b[A\n",
      " 10%|████▌                                       | 3/29 [00:03<00:30,  1.19s/it]\u001b[A\n",
      " 14%|██████                                      | 4/29 [00:05<00:34,  1.37s/it]\u001b[A\n",
      " 17%|███████▌                                    | 5/29 [00:06<00:35,  1.48s/it]\u001b[A\n",
      " 21%|█████████                                   | 6/29 [00:08<00:35,  1.55s/it]\u001b[A\n",
      " 24%|██████████▌                                 | 7/29 [00:10<00:34,  1.59s/it]\u001b[A\n",
      " 28%|████████████▏                               | 8/29 [00:11<00:33,  1.62s/it]\u001b[A\n",
      " 31%|█████████████▋                              | 9/29 [00:13<00:32,  1.64s/it]\u001b[A\n",
      " 34%|██████████████▊                            | 10/29 [00:15<00:31,  1.65s/it]\u001b[A\n",
      " 38%|████████████████▎                          | 11/29 [00:16<00:29,  1.66s/it]\u001b[A\n",
      " 41%|█████████████████▊                         | 12/29 [00:18<00:28,  1.67s/it]\u001b[A\n",
      " 45%|███████████████████▎                       | 13/29 [00:20<00:26,  1.67s/it]\u001b[A\n",
      " 48%|████████████████████▊                      | 14/29 [00:21<00:25,  1.67s/it]\u001b[A\n",
      " 52%|██████████████████████▏                    | 15/29 [00:23<00:23,  1.68s/it]\u001b[A\n",
      " 55%|███████████████████████▋                   | 16/29 [00:25<00:21,  1.68s/it]\u001b[A\n",
      " 59%|█████████████████████████▏                 | 17/29 [00:26<00:20,  1.68s/it]\u001b[A\n",
      " 62%|██████████████████████████▋                | 18/29 [00:28<00:18,  1.68s/it]\u001b[A\n",
      " 66%|████████████████████████████▏              | 19/29 [00:30<00:16,  1.68s/it]\u001b[A\n",
      " 69%|█████████████████████████████▋             | 20/29 [00:31<00:15,  1.68s/it]\u001b[A\n",
      " 72%|███████████████████████████████▏           | 21/29 [00:33<00:13,  1.68s/it]\u001b[A\n",
      " 76%|████████████████████████████████▌          | 22/29 [00:35<00:11,  1.68s/it]\u001b[A\n",
      " 79%|██████████████████████████████████         | 23/29 [00:36<00:10,  1.68s/it]\u001b[A\n",
      " 83%|███████████████████████████████████▌       | 24/29 [00:38<00:08,  1.68s/it]\u001b[A\n",
      " 86%|█████████████████████████████████████      | 25/29 [00:40<00:06,  1.68s/it]\u001b[A\n",
      " 90%|██████████████████████████████████████▌    | 26/29 [00:42<00:05,  1.68s/it]\u001b[A\n",
      " 93%|████████████████████████████████████████   | 27/29 [00:43<00:03,  1.68s/it]\u001b[A\n",
      " 97%|█████████████████████████████████████████▌ | 28/29 [00:45<00:01,  1.68s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.1258491277694702, 'eval_runtime': 48.7712, 'eval_samples_per_second': 2.378, 'eval_steps_per_second': 0.595, 'epoch': 1.0}\n",
      " 20%|███████▌                              | 233/1165 [21:22<1:22:38,  5.32s/it]\n",
      "100%|███████████████████████████████████████████| 29/29 [00:47<00:00,  1.68s/it]\u001b[A\n",
      "                                                                                \u001b[A/ext3/miniconda3/envs/owl-botu/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "{'loss': 1.1601, 'learning_rate': 2.6622137404580153e-05, 'epoch': 1.01}        \n",
      "{'loss': 1.148, 'learning_rate': 2.6479007633587788e-05, 'epoch': 1.03}         \n",
      "{'loss': 1.1629, 'learning_rate': 2.6335877862595423e-05, 'epoch': 1.05}        \n",
      "{'loss': 1.1493, 'learning_rate': 2.6192748091603054e-05, 'epoch': 1.07}        \n",
      "{'loss': 1.0878, 'learning_rate': 2.604961832061069e-05, 'epoch': 1.09}         \n",
      "{'loss': 1.1536, 'learning_rate': 2.590648854961832e-05, 'epoch': 1.12}         \n",
      "{'loss': 1.1013, 'learning_rate': 2.5763358778625956e-05, 'epoch': 1.14}        \n",
      "{'loss': 1.179, 'learning_rate': 2.562022900763359e-05, 'epoch': 1.16}          \n",
      "{'loss': 1.188, 'learning_rate': 2.5477099236641223e-05, 'epoch': 1.18}         \n",
      "{'loss': 1.118, 'learning_rate': 2.5333969465648855e-05, 'epoch': 1.2}          \n",
      "{'loss': 1.1282, 'learning_rate': 2.5190839694656487e-05, 'epoch': 1.22}        \n",
      "{'loss': 1.1354, 'learning_rate': 2.5047709923664122e-05, 'epoch': 1.24}        \n",
      "{'loss': 1.1424, 'learning_rate': 2.4904580152671757e-05, 'epoch': 1.27}        \n",
      "{'loss': 1.1215, 'learning_rate': 2.476145038167939e-05, 'epoch': 1.29}         \n",
      "{'loss': 1.1176, 'learning_rate': 2.4618320610687024e-05, 'epoch': 1.31}        \n",
      "{'loss': 1.0676, 'learning_rate': 2.4475190839694655e-05, 'epoch': 1.33}        \n",
      "{'loss': 1.0984, 'learning_rate': 2.433206106870229e-05, 'epoch': 1.35}         \n",
      "{'loss': 1.0668, 'learning_rate': 2.4188931297709926e-05, 'epoch': 1.37}        \n",
      "{'loss': 1.1243, 'learning_rate': 2.4045801526717557e-05, 'epoch': 1.39}        \n",
      "{'loss': 1.1639, 'learning_rate': 2.3902671755725192e-05, 'epoch': 1.42}        \n",
      "{'loss': 1.074, 'learning_rate': 2.3759541984732824e-05, 'epoch': 1.44}         \n",
      "{'loss': 1.0848, 'learning_rate': 2.361641221374046e-05, 'epoch': 1.46}         \n",
      "{'loss': 1.0973, 'learning_rate': 2.3473282442748094e-05, 'epoch': 1.48}        \n",
      "{'loss': 1.0787, 'learning_rate': 2.3330152671755726e-05, 'epoch': 1.5}         \n",
      "{'loss': 1.1339, 'learning_rate': 2.318702290076336e-05, 'epoch': 1.52}         \n",
      "{'loss': 1.0994, 'learning_rate': 2.3043893129770993e-05, 'epoch': 1.55}        \n",
      "{'loss': 1.1225, 'learning_rate': 2.2900763358778628e-05, 'epoch': 1.57}        \n",
      "{'loss': 1.1008, 'learning_rate': 2.275763358778626e-05, 'epoch': 1.59}         \n",
      "{'loss': 1.1585, 'learning_rate': 2.261450381679389e-05, 'epoch': 1.61}         \n",
      "{'loss': 1.0399, 'learning_rate': 2.2471374045801526e-05, 'epoch': 1.63}        \n",
      "{'loss': 1.1152, 'learning_rate': 2.2328244274809158e-05, 'epoch': 1.65}        \n",
      "{'loss': 1.1439, 'learning_rate': 2.2185114503816793e-05, 'epoch': 1.67}        \n",
      "{'loss': 1.1334, 'learning_rate': 2.204198473282443e-05, 'epoch': 1.7}          \n",
      "{'loss': 1.1214, 'learning_rate': 2.189885496183206e-05, 'epoch': 1.72}         \n",
      "{'loss': 1.1721, 'learning_rate': 2.1755725190839695e-05, 'epoch': 1.74}        \n",
      "{'loss': 1.0989, 'learning_rate': 2.161259541984733e-05, 'epoch': 1.76}         \n",
      "{'loss': 1.1005, 'learning_rate': 2.1469465648854962e-05, 'epoch': 1.78}        \n",
      "{'loss': 1.1358, 'learning_rate': 2.1326335877862597e-05, 'epoch': 1.8}         \n",
      "{'loss': 1.1, 'learning_rate': 2.118320610687023e-05, 'epoch': 1.82}            \n",
      "{'loss': 1.0795, 'learning_rate': 2.1040076335877864e-05, 'epoch': 1.85}        \n",
      "{'loss': 1.0683, 'learning_rate': 2.08969465648855e-05, 'epoch': 1.87}          \n",
      "{'loss': 1.0895, 'learning_rate': 2.075381679389313e-05, 'epoch': 1.89}         \n",
      "{'loss': 1.0326, 'learning_rate': 2.0610687022900766e-05, 'epoch': 1.91}        \n",
      "{'loss': 1.0999, 'learning_rate': 2.0467557251908397e-05, 'epoch': 1.93}        \n",
      "{'loss': 1.0899, 'learning_rate': 2.0324427480916033e-05, 'epoch': 1.95}        \n",
      "{'loss': 1.1061, 'learning_rate': 2.0181297709923668e-05, 'epoch': 1.97}        \n",
      "{'loss': 1.079, 'learning_rate': 2.0038167938931296e-05, 'epoch': 2.0}          \n",
      " 40%|███████████████▏                      | 466/1165 [42:01<1:01:43,  5.30s/it]\n",
      "  0%|                                                    | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "  7%|███                                         | 2/29 [00:01<00:22,  1.20it/s]\u001b[A\n",
      " 10%|████▌                                       | 3/29 [00:03<00:30,  1.18s/it]\u001b[A\n",
      " 14%|██████                                      | 4/29 [00:05<00:34,  1.37s/it]\u001b[A\n",
      " 17%|███████▌                                    | 5/29 [00:06<00:35,  1.47s/it]\u001b[A\n",
      " 21%|█████████                                   | 6/29 [00:08<00:35,  1.54s/it]\u001b[A\n",
      " 24%|██████████▌                                 | 7/29 [00:10<00:34,  1.58s/it]\u001b[A\n",
      " 28%|████████████▏                               | 8/29 [00:11<00:33,  1.61s/it]\u001b[A\n",
      " 31%|█████████████▋                              | 9/29 [00:13<00:32,  1.63s/it]\u001b[A\n",
      " 34%|██████████████▊                            | 10/29 [00:15<00:31,  1.64s/it]\u001b[A\n",
      " 38%|████████████████▎                          | 11/29 [00:16<00:29,  1.65s/it]\u001b[A\n",
      " 41%|█████████████████▊                         | 12/29 [00:18<00:28,  1.66s/it]\u001b[A\n",
      " 45%|███████████████████▎                       | 13/29 [00:20<00:26,  1.66s/it]\u001b[A\n",
      " 48%|████████████████████▊                      | 14/29 [00:21<00:24,  1.67s/it]\u001b[A\n",
      " 52%|██████████████████████▏                    | 15/29 [00:23<00:23,  1.67s/it]\u001b[A\n",
      " 55%|███████████████████████▋                   | 16/29 [00:25<00:21,  1.67s/it]\u001b[A\n",
      " 59%|█████████████████████████▏                 | 17/29 [00:26<00:20,  1.67s/it]\u001b[A\n",
      " 62%|██████████████████████████▋                | 18/29 [00:28<00:18,  1.67s/it]\u001b[A\n",
      " 66%|████████████████████████████▏              | 19/29 [00:30<00:16,  1.67s/it]\u001b[A\n",
      " 69%|█████████████████████████████▋             | 20/29 [00:31<00:15,  1.67s/it]\u001b[A\n",
      " 72%|███████████████████████████████▏           | 21/29 [00:33<00:13,  1.67s/it]\u001b[A\n",
      " 76%|████████████████████████████████▌          | 22/29 [00:35<00:11,  1.67s/it]\u001b[A\n",
      " 79%|██████████████████████████████████         | 23/29 [00:36<00:10,  1.67s/it]\u001b[A\n",
      " 83%|███████████████████████████████████▌       | 24/29 [00:38<00:08,  1.67s/it]\u001b[A\n",
      " 86%|█████████████████████████████████████      | 25/29 [00:40<00:06,  1.67s/it]\u001b[A\n",
      " 90%|██████████████████████████████████████▌    | 26/29 [00:41<00:05,  1.67s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 93%|████████████████████████████████████████   | 27/29 [00:43<00:03,  1.67s/it]\u001b[A\n",
      " 97%|█████████████████████████████████████████▌ | 28/29 [00:45<00:01,  1.67s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.0947836637496948, 'eval_runtime': 48.521, 'eval_samples_per_second': 2.391, 'eval_steps_per_second': 0.598, 'epoch': 2.0}\n",
      " 40%|███████████████▏                      | 466/1165 [42:49<1:01:43,  5.30s/it]\n",
      "100%|███████████████████████████████████████████| 29/29 [00:47<00:00,  1.67s/it]\u001b[A\n",
      "                                                                                \u001b[A/ext3/miniconda3/envs/owl-botu/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "{'loss': 1.1204, 'learning_rate': 1.989503816793893e-05, 'epoch': 2.02}         \n",
      "{'loss': 1.1121, 'learning_rate': 1.9751908396946563e-05, 'epoch': 2.04}        \n",
      "{'loss': 1.0879, 'learning_rate': 1.9608778625954198e-05, 'epoch': 2.06}        \n",
      "{'loss': 1.0865, 'learning_rate': 1.9465648854961833e-05, 'epoch': 2.08}        \n",
      "{'loss': 1.1075, 'learning_rate': 1.9322519083969465e-05, 'epoch': 2.1}         \n",
      "{'loss': 1.0516, 'learning_rate': 1.91793893129771e-05, 'epoch': 2.12}          \n",
      "{'loss': 1.1232, 'learning_rate': 1.903625954198473e-05, 'epoch': 2.15}         \n",
      "{'loss': 1.0755, 'learning_rate': 1.8893129770992367e-05, 'epoch': 2.17}        \n",
      "{'loss': 1.1066, 'learning_rate': 1.8750000000000002e-05, 'epoch': 2.19}        \n",
      "{'loss': 1.0596, 'learning_rate': 1.8606870229007633e-05, 'epoch': 2.21}        \n",
      "{'loss': 1.0821, 'learning_rate': 1.846374045801527e-05, 'epoch': 2.23}         \n",
      "{'loss': 1.1245, 'learning_rate': 1.83206106870229e-05, 'epoch': 2.25}          \n",
      "{'loss': 1.1073, 'learning_rate': 1.8177480916030535e-05, 'epoch': 2.27}        \n",
      "{'loss': 1.1222, 'learning_rate': 1.803435114503817e-05, 'epoch': 2.3}          \n",
      "{'loss': 1.0895, 'learning_rate': 1.7891221374045802e-05, 'epoch': 2.32}        \n",
      "{'loss': 1.0837, 'learning_rate': 1.7748091603053437e-05, 'epoch': 2.34}        \n",
      "{'loss': 1.0435, 'learning_rate': 1.760496183206107e-05, 'epoch': 2.36}         \n",
      "{'loss': 1.1002, 'learning_rate': 1.7461832061068704e-05, 'epoch': 2.38}        \n",
      "{'loss': 1.1351, 'learning_rate': 1.7318702290076336e-05, 'epoch': 2.4}         \n",
      "{'loss': 1.1621, 'learning_rate': 1.7175572519083968e-05, 'epoch': 2.42}        \n",
      "{'loss': 1.0555, 'learning_rate': 1.7032442748091603e-05, 'epoch': 2.45}        \n",
      "{'loss': 1.126, 'learning_rate': 1.6889312977099238e-05, 'epoch': 2.47}         \n",
      "{'loss': 1.0874, 'learning_rate': 1.674618320610687e-05, 'epoch': 2.49}         \n",
      "{'loss': 1.0115, 'learning_rate': 1.6603053435114505e-05, 'epoch': 2.51}        \n",
      "{'loss': 1.1743, 'learning_rate': 1.6459923664122136e-05, 'epoch': 2.53}        \n",
      "{'loss': 1.1047, 'learning_rate': 1.631679389312977e-05, 'epoch': 2.55}         \n",
      "{'loss': 1.0914, 'learning_rate': 1.6173664122137406e-05, 'epoch': 2.58}        \n",
      "{'loss': 1.0079, 'learning_rate': 1.6030534351145038e-05, 'epoch': 2.6}         \n",
      "{'loss': 1.0662, 'learning_rate': 1.5887404580152673e-05, 'epoch': 2.62}        \n",
      "{'loss': 1.1087, 'learning_rate': 1.5744274809160305e-05, 'epoch': 2.64}        \n",
      "{'loss': 1.1152, 'learning_rate': 1.560114503816794e-05, 'epoch': 2.66}         \n",
      "{'loss': 1.0791, 'learning_rate': 1.5458015267175575e-05, 'epoch': 2.68}        \n",
      "{'loss': 1.0444, 'learning_rate': 1.5314885496183207e-05, 'epoch': 2.7}         \n",
      "{'loss': 1.0935, 'learning_rate': 1.5171755725190842e-05, 'epoch': 2.73}        \n",
      "{'loss': 1.0896, 'learning_rate': 1.5028625954198475e-05, 'epoch': 2.75}        \n",
      "{'loss': 1.0803, 'learning_rate': 1.4885496183206107e-05, 'epoch': 2.77}        \n",
      "{'loss': 1.0573, 'learning_rate': 1.474236641221374e-05, 'epoch': 2.79}         \n",
      "{'loss': 1.0707, 'learning_rate': 1.4599236641221374e-05, 'epoch': 2.81}        \n",
      "{'loss': 1.0715, 'learning_rate': 1.4456106870229009e-05, 'epoch': 2.83}        \n",
      "{'loss': 1.1072, 'learning_rate': 1.4312977099236642e-05, 'epoch': 2.85}        \n",
      "{'loss': 1.1284, 'learning_rate': 1.4169847328244276e-05, 'epoch': 2.88}        \n",
      "{'loss': 1.0897, 'learning_rate': 1.4026717557251908e-05, 'epoch': 2.9}         \n",
      "{'loss': 1.0996, 'learning_rate': 1.3883587786259541e-05, 'epoch': 2.92}        \n",
      "{'loss': 1.0794, 'learning_rate': 1.3740458015267176e-05, 'epoch': 2.94}        \n",
      "{'loss': 1.1049, 'learning_rate': 1.359732824427481e-05, 'epoch': 2.96}         \n",
      "{'loss': 1.0811, 'learning_rate': 1.3454198473282443e-05, 'epoch': 2.98}        \n",
      " 60%|██████████████████████▊               | 699/1165 [1:03:29<41:19,  5.32s/it]\n",
      "  0%|                                                    | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "  7%|███                                         | 2/29 [00:01<00:22,  1.19it/s]\u001b[A\n",
      " 10%|████▌                                       | 3/29 [00:03<00:30,  1.19s/it]\u001b[A\n",
      " 14%|██████                                      | 4/29 [00:05<00:34,  1.37s/it]\u001b[A\n",
      " 17%|███████▌                                    | 5/29 [00:06<00:35,  1.48s/it]\u001b[A\n",
      " 21%|█████████                                   | 6/29 [00:08<00:35,  1.55s/it]\u001b[A\n",
      " 24%|██████████▌                                 | 7/29 [00:10<00:34,  1.59s/it]\u001b[A\n",
      " 28%|████████████▏                               | 8/29 [00:11<00:34,  1.62s/it]\u001b[A\n",
      " 31%|█████████████▋                              | 9/29 [00:13<00:32,  1.64s/it]\u001b[A\n",
      " 34%|██████████████▊                            | 10/29 [00:15<00:31,  1.65s/it]\u001b[A\n",
      " 38%|████████████████▎                          | 11/29 [00:16<00:29,  1.66s/it]\u001b[A\n",
      " 41%|█████████████████▊                         | 12/29 [00:18<00:28,  1.67s/it]\u001b[A\n",
      " 45%|███████████████████▎                       | 13/29 [00:20<00:26,  1.67s/it]\u001b[A\n",
      " 48%|████████████████████▊                      | 14/29 [00:21<00:25,  1.67s/it]\u001b[A\n",
      " 52%|██████████████████████▏                    | 15/29 [00:23<00:23,  1.68s/it]\u001b[A\n",
      " 55%|███████████████████████▋                   | 16/29 [00:25<00:21,  1.68s/it]\u001b[A\n",
      " 59%|█████████████████████████▏                 | 17/29 [00:26<00:20,  1.68s/it]\u001b[A\n",
      " 62%|██████████████████████████▋                | 18/29 [00:28<00:18,  1.68s/it]\u001b[A\n",
      " 66%|████████████████████████████▏              | 19/29 [00:30<00:16,  1.68s/it]\u001b[A\n",
      " 69%|█████████████████████████████▋             | 20/29 [00:31<00:15,  1.68s/it]\u001b[A\n",
      " 72%|███████████████████████████████▏           | 21/29 [00:33<00:13,  1.68s/it]\u001b[A\n",
      " 76%|████████████████████████████████▌          | 22/29 [00:35<00:11,  1.68s/it]\u001b[A\n",
      " 79%|██████████████████████████████████         | 23/29 [00:36<00:10,  1.68s/it]\u001b[A\n",
      " 83%|███████████████████████████████████▌       | 24/29 [00:38<00:08,  1.68s/it]\u001b[A\n",
      " 86%|█████████████████████████████████████      | 25/29 [00:40<00:06,  1.68s/it]\u001b[A\n",
      " 90%|██████████████████████████████████████▌    | 26/29 [00:42<00:05,  1.68s/it]\u001b[A\n",
      " 93%|████████████████████████████████████████   | 27/29 [00:43<00:03,  1.68s/it]\u001b[A\n",
      " 97%|█████████████████████████████████████████▌ | 28/29 [00:45<00:01,  1.68s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.0845725536346436, 'eval_runtime': 48.766, 'eval_samples_per_second': 2.379, 'eval_steps_per_second': 0.595, 'epoch': 3.0}\n",
      " 60%|██████████████████████▊               | 699/1165 [1:04:18<41:19,  5.32s/it]\n",
      "100%|███████████████████████████████████████████| 29/29 [00:47<00:00,  1.68s/it]\u001b[A\n",
      "                                                                                \u001b[A/ext3/miniconda3/envs/owl-botu/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.041, 'learning_rate': 1.3311068702290076e-05, 'epoch': 3.0}          \n",
      "{'loss': 1.0221, 'learning_rate': 1.3167938931297711e-05, 'epoch': 3.03}        \n",
      "{'loss': 1.129, 'learning_rate': 1.3024809160305345e-05, 'epoch': 3.05}         \n",
      "{'loss': 1.0891, 'learning_rate': 1.2881679389312978e-05, 'epoch': 3.07}        \n",
      "{'loss': 1.111, 'learning_rate': 1.2738549618320612e-05, 'epoch': 3.09}         \n",
      "{'loss': 1.0935, 'learning_rate': 1.2595419847328243e-05, 'epoch': 3.11}        \n",
      "{'loss': 1.0385, 'learning_rate': 1.2452290076335878e-05, 'epoch': 3.13}        \n",
      "{'loss': 1.1061, 'learning_rate': 1.2309160305343512e-05, 'epoch': 3.15}        \n",
      "{'loss': 1.1058, 'learning_rate': 1.2166030534351145e-05, 'epoch': 3.18}        \n",
      "{'loss': 1.0844, 'learning_rate': 1.2022900763358779e-05, 'epoch': 3.2}         \n",
      "{'loss': 1.091, 'learning_rate': 1.1879770992366412e-05, 'epoch': 3.22}         \n",
      "{'loss': 1.1124, 'learning_rate': 1.1736641221374047e-05, 'epoch': 3.24}        \n",
      "{'loss': 1.1026, 'learning_rate': 1.159351145038168e-05, 'epoch': 3.26}         \n",
      "{'loss': 1.0751, 'learning_rate': 1.1450381679389314e-05, 'epoch': 3.28}        \n",
      "{'loss': 1.0007, 'learning_rate': 1.1307251908396946e-05, 'epoch': 3.3}         \n",
      "{'loss': 1.1078, 'learning_rate': 1.1164122137404579e-05, 'epoch': 3.33}        \n",
      "{'loss': 1.0849, 'learning_rate': 1.1020992366412214e-05, 'epoch': 3.35}        \n",
      "{'loss': 1.0926, 'learning_rate': 1.0877862595419848e-05, 'epoch': 3.37}        \n",
      "{'loss': 1.0748, 'learning_rate': 1.0734732824427481e-05, 'epoch': 3.39}        \n",
      "{'loss': 1.1014, 'learning_rate': 1.0591603053435114e-05, 'epoch': 3.41}        \n",
      "{'loss': 1.1043, 'learning_rate': 1.044847328244275e-05, 'epoch': 3.43}         \n",
      "{'loss': 1.1079, 'learning_rate': 1.0305343511450383e-05, 'epoch': 3.45}        \n",
      "{'loss': 1.0193, 'learning_rate': 1.0162213740458016e-05, 'epoch': 3.48}        \n",
      "{'loss': 1.0877, 'learning_rate': 1.0019083969465648e-05, 'epoch': 3.5}         \n",
      "{'loss': 1.0571, 'learning_rate': 9.875954198473281e-06, 'epoch': 3.52}         \n",
      "{'loss': 1.0519, 'learning_rate': 9.732824427480917e-06, 'epoch': 3.54}         \n",
      "{'loss': 1.0797, 'learning_rate': 9.58969465648855e-06, 'epoch': 3.56}          \n",
      "{'loss': 1.0973, 'learning_rate': 9.446564885496183e-06, 'epoch': 3.58}         \n",
      "{'loss': 1.0532, 'learning_rate': 9.303435114503817e-06, 'epoch': 3.61}         \n",
      "{'loss': 1.0995, 'learning_rate': 9.16030534351145e-06, 'epoch': 3.63}          \n",
      "{'loss': 1.039, 'learning_rate': 9.017175572519085e-06, 'epoch': 3.65}          \n",
      "{'loss': 1.0832, 'learning_rate': 8.874045801526719e-06, 'epoch': 3.67}         \n",
      "{'loss': 1.0825, 'learning_rate': 8.730916030534352e-06, 'epoch': 3.69}         \n",
      "{'loss': 1.0679, 'learning_rate': 8.587786259541984e-06, 'epoch': 3.71}         \n",
      "{'loss': 1.0271, 'learning_rate': 8.444656488549619e-06, 'epoch': 3.73}         \n",
      "{'loss': 1.0409, 'learning_rate': 8.301526717557252e-06, 'epoch': 3.76}         \n",
      "{'loss': 1.1157, 'learning_rate': 8.158396946564886e-06, 'epoch': 3.78}         \n",
      "{'loss': 1.1488, 'learning_rate': 8.015267175572519e-06, 'epoch': 3.8}          \n",
      "{'loss': 1.0826, 'learning_rate': 7.872137404580152e-06, 'epoch': 3.82}         \n",
      "{'loss': 1.0207, 'learning_rate': 7.729007633587788e-06, 'epoch': 3.84}         \n",
      "{'loss': 1.0407, 'learning_rate': 7.585877862595421e-06, 'epoch': 3.86}         \n",
      "{'loss': 1.1144, 'learning_rate': 7.4427480916030536e-06, 'epoch': 3.88}        \n",
      "{'loss': 1.038, 'learning_rate': 7.299618320610687e-06, 'epoch': 3.91}          \n",
      "{'loss': 1.1266, 'learning_rate': 7.156488549618321e-06, 'epoch': 3.93}         \n",
      "{'loss': 1.0916, 'learning_rate': 7.013358778625954e-06, 'epoch': 3.95}         \n",
      "{'loss': 1.1145, 'learning_rate': 6.870229007633588e-06, 'epoch': 3.97}         \n",
      "{'loss': 1.0704, 'learning_rate': 6.7270992366412214e-06, 'epoch': 3.99}        \n",
      " 80%|██████████████████████████████▍       | 932/1165 [1:24:57<20:37,  5.31s/it]\n",
      "  0%|                                                    | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "  7%|███                                         | 2/29 [00:01<00:22,  1.19it/s]\u001b[A\n",
      " 10%|████▌                                       | 3/29 [00:03<00:30,  1.19s/it]\u001b[A\n",
      " 14%|██████                                      | 4/29 [00:05<00:34,  1.37s/it]\u001b[A\n",
      " 17%|███████▌                                    | 5/29 [00:06<00:35,  1.48s/it]\u001b[A\n",
      " 21%|█████████                                   | 6/29 [00:08<00:35,  1.54s/it]\u001b[A\n",
      " 24%|██████████▌                                 | 7/29 [00:10<00:34,  1.59s/it]\u001b[A\n",
      " 28%|████████████▏                               | 8/29 [00:11<00:33,  1.61s/it]\u001b[A\n",
      " 31%|█████████████▋                              | 9/29 [00:13<00:32,  1.63s/it]\u001b[A\n",
      " 34%|██████████████▊                            | 10/29 [00:15<00:31,  1.65s/it]\u001b[A\n",
      " 38%|████████████████▎                          | 11/29 [00:16<00:29,  1.66s/it]\u001b[A\n",
      " 41%|█████████████████▊                         | 12/29 [00:18<00:28,  1.66s/it]\u001b[A\n",
      " 45%|███████████████████▎                       | 13/29 [00:20<00:26,  1.67s/it]\u001b[A\n",
      " 48%|████████████████████▊                      | 14/29 [00:21<00:25,  1.67s/it]\u001b[A\n",
      " 52%|██████████████████████▏                    | 15/29 [00:23<00:23,  1.67s/it]\u001b[A\n",
      " 55%|███████████████████████▋                   | 16/29 [00:25<00:21,  1.67s/it]\u001b[A\n",
      " 59%|█████████████████████████▏                 | 17/29 [00:26<00:20,  1.67s/it]\u001b[A\n",
      " 62%|██████████████████████████▋                | 18/29 [00:28<00:18,  1.68s/it]\u001b[A\n",
      " 66%|████████████████████████████▏              | 19/29 [00:30<00:16,  1.68s/it]\u001b[A\n",
      " 69%|█████████████████████████████▋             | 20/29 [00:31<00:15,  1.68s/it]\u001b[A\n",
      " 72%|███████████████████████████████▏           | 21/29 [00:33<00:13,  1.68s/it]\u001b[A\n",
      " 76%|████████████████████████████████▌          | 22/29 [00:35<00:11,  1.68s/it]\u001b[A\n",
      " 79%|██████████████████████████████████         | 23/29 [00:36<00:10,  1.68s/it]\u001b[A\n",
      " 83%|███████████████████████████████████▌       | 24/29 [00:38<00:08,  1.68s/it]\u001b[A\n",
      " 86%|█████████████████████████████████████      | 25/29 [00:40<00:06,  1.68s/it]\u001b[A\n",
      " 90%|██████████████████████████████████████▌    | 26/29 [00:41<00:05,  1.68s/it]\u001b[A\n",
      " 93%|████████████████████████████████████████   | 27/29 [00:43<00:03,  1.68s/it]\u001b[A\n",
      " 97%|█████████████████████████████████████████▌ | 28/29 [00:45<00:01,  1.68s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.0802675485610962, 'eval_runtime': 48.6409, 'eval_samples_per_second': 2.385, 'eval_steps_per_second': 0.596, 'epoch': 4.0}\n",
      " 80%|██████████████████████████████▍       | 932/1165 [1:25:46<20:37,  5.31s/it]\n",
      "100%|███████████████████████████████████████████| 29/29 [00:47<00:00,  1.68s/it]\u001b[A\n",
      "                                                                                \u001b[A/ext3/miniconda3/envs/owl-botu/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "{'loss': 1.1024, 'learning_rate': 6.583969465648856e-06, 'epoch': 4.01}         \n",
      "{'loss': 1.0703, 'learning_rate': 6.440839694656489e-06, 'epoch': 4.03}         \n",
      "{'loss': 1.033, 'learning_rate': 6.297709923664122e-06, 'epoch': 4.06}          \n",
      "{'loss': 1.1117, 'learning_rate': 6.154580152671756e-06, 'epoch': 4.08}         \n",
      "{'loss': 1.0675, 'learning_rate': 6.011450381679389e-06, 'epoch': 4.1}          \n",
      "{'loss': 1.0348, 'learning_rate': 5.8683206106870236e-06, 'epoch': 4.12}        \n",
      "{'loss': 1.1296, 'learning_rate': 5.725190839694657e-06, 'epoch': 4.14}         \n",
      "{'loss': 1.013, 'learning_rate': 5.5820610687022895e-06, 'epoch': 4.16}         \n",
      "{'loss': 1.0977, 'learning_rate': 5.438931297709924e-06, 'epoch': 4.18}         \n",
      "{'loss': 1.0439, 'learning_rate': 5.295801526717557e-06, 'epoch': 4.21}         \n",
      "{'loss': 1.0427, 'learning_rate': 5.1526717557251914e-06, 'epoch': 4.23}        \n",
      "{'loss': 1.0381, 'learning_rate': 5.009541984732824e-06, 'epoch': 4.25}         \n",
      "{'loss': 1.0866, 'learning_rate': 4.866412213740458e-06, 'epoch': 4.27}         \n",
      "{'loss': 1.1062, 'learning_rate': 4.723282442748092e-06, 'epoch': 4.29}         \n",
      "{'loss': 1.0827, 'learning_rate': 4.580152671755725e-06, 'epoch': 4.31}         \n",
      "{'loss': 1.0743, 'learning_rate': 4.437022900763359e-06, 'epoch': 4.33}         \n",
      "{'loss': 1.0572, 'learning_rate': 4.293893129770992e-06, 'epoch': 4.36}         \n",
      "{'loss': 1.1268, 'learning_rate': 4.150763358778626e-06, 'epoch': 4.38}         \n",
      "{'loss': 1.043, 'learning_rate': 4.0076335877862595e-06, 'epoch': 4.4}          \n",
      "{'loss': 1.1057, 'learning_rate': 3.864503816793894e-06, 'epoch': 4.42}         \n",
      "{'loss': 1.0379, 'learning_rate': 3.7213740458015268e-06, 'epoch': 4.44}        \n",
      "{'loss': 1.0559, 'learning_rate': 3.5782442748091606e-06, 'epoch': 4.46}        \n",
      "{'loss': 1.0672, 'learning_rate': 3.435114503816794e-06, 'epoch': 4.48}         \n",
      "{'loss': 1.075, 'learning_rate': 3.291984732824428e-06, 'epoch': 4.51}          \n",
      "{'loss': 1.1352, 'learning_rate': 3.148854961832061e-06, 'epoch': 4.53}         \n",
      "{'loss': 1.0587, 'learning_rate': 3.0057251908396947e-06, 'epoch': 4.55}        \n",
      "{'loss': 1.1049, 'learning_rate': 2.8625954198473285e-06, 'epoch': 4.57}        \n",
      "{'loss': 0.992, 'learning_rate': 2.719465648854962e-06, 'epoch': 4.59}          \n",
      "{'loss': 1.0882, 'learning_rate': 2.5763358778625957e-06, 'epoch': 4.61}        \n",
      "{'loss': 1.0646, 'learning_rate': 2.433206106870229e-06, 'epoch': 4.64}         \n",
      "{'loss': 1.1308, 'learning_rate': 2.2900763358778625e-06, 'epoch': 4.66}        \n",
      "{'loss': 1.0182, 'learning_rate': 2.146946564885496e-06, 'epoch': 4.68}         \n",
      "{'loss': 1.0805, 'learning_rate': 2.0038167938931298e-06, 'epoch': 4.7}         \n",
      "{'loss': 1.0754, 'learning_rate': 1.8606870229007634e-06, 'epoch': 4.72}        \n",
      "{'loss': 1.0386, 'learning_rate': 1.717557251908397e-06, 'epoch': 4.74}         \n",
      "{'loss': 1.1389, 'learning_rate': 1.5744274809160304e-06, 'epoch': 4.76}        \n",
      "{'loss': 1.1049, 'learning_rate': 1.4312977099236642e-06, 'epoch': 4.79}        \n",
      "{'loss': 1.081, 'learning_rate': 1.2881679389312979e-06, 'epoch': 4.81}         \n",
      "{'loss': 1.0207, 'learning_rate': 1.1450381679389313e-06, 'epoch': 4.83}        \n",
      "{'loss': 1.1211, 'learning_rate': 1.0019083969465649e-06, 'epoch': 4.85}        \n",
      "{'loss': 1.1196, 'learning_rate': 8.587786259541985e-07, 'epoch': 4.87}         \n",
      "{'loss': 1.1041, 'learning_rate': 7.156488549618321e-07, 'epoch': 4.89}         \n",
      "{'loss': 1.0672, 'learning_rate': 5.725190839694656e-07, 'epoch': 4.91}         \n",
      "{'loss': 1.0198, 'learning_rate': 4.2938931297709925e-07, 'epoch': 4.94}        \n",
      "{'loss': 1.0996, 'learning_rate': 2.862595419847328e-07, 'epoch': 4.96}         \n",
      "{'loss': 1.0485, 'learning_rate': 1.431297709923664e-07, 'epoch': 4.98}         \n",
      "{'loss': 1.0872, 'learning_rate': 0.0, 'epoch': 5.0}                            \n",
      "100%|█████████████████████████████████████| 1165/1165 [1:46:25<00:00,  5.31s/it]\n",
      "  0%|                                                    | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "  7%|███                                         | 2/29 [00:01<00:22,  1.19it/s]\u001b[A\n",
      " 10%|████▌                                       | 3/29 [00:03<00:30,  1.19s/it]\u001b[A\n",
      " 14%|██████                                      | 4/29 [00:05<00:34,  1.37s/it]\u001b[A\n",
      " 17%|███████▌                                    | 5/29 [00:06<00:35,  1.48s/it]\u001b[A\n",
      " 21%|█████████                                   | 6/29 [00:08<00:35,  1.54s/it]\u001b[A\n",
      " 24%|██████████▌                                 | 7/29 [00:10<00:34,  1.59s/it]\u001b[A\n",
      " 28%|████████████▏                               | 8/29 [00:11<00:33,  1.62s/it]\u001b[A\n",
      " 31%|█████████████▋                              | 9/29 [00:13<00:32,  1.63s/it]\u001b[A\n",
      " 34%|██████████████▊                            | 10/29 [00:15<00:31,  1.65s/it]\u001b[A\n",
      " 38%|████████████████▎                          | 11/29 [00:16<00:29,  1.66s/it]\u001b[A\n",
      " 41%|█████████████████▊                         | 12/29 [00:18<00:28,  1.66s/it]\u001b[A\n",
      " 45%|███████████████████▎                       | 13/29 [00:20<00:26,  1.67s/it]\u001b[A\n",
      " 48%|████████████████████▊                      | 14/29 [00:21<00:25,  1.67s/it]\u001b[A\n",
      " 52%|██████████████████████▏                    | 15/29 [00:23<00:23,  1.67s/it]\u001b[A\n",
      " 55%|███████████████████████▋                   | 16/29 [00:25<00:21,  1.67s/it]\u001b[A\n",
      " 59%|█████████████████████████▏                 | 17/29 [00:26<00:20,  1.68s/it]\u001b[A\n",
      " 62%|██████████████████████████▋                | 18/29 [00:28<00:18,  1.68s/it]\u001b[A\n",
      " 66%|████████████████████████████▏              | 19/29 [00:30<00:16,  1.68s/it]\u001b[A\n",
      " 69%|█████████████████████████████▋             | 20/29 [00:31<00:15,  1.68s/it]\u001b[A\n",
      " 72%|███████████████████████████████▏           | 21/29 [00:33<00:13,  1.68s/it]\u001b[A\n",
      " 76%|████████████████████████████████▌          | 22/29 [00:35<00:11,  1.68s/it]\u001b[A\n",
      " 79%|██████████████████████████████████         | 23/29 [00:36<00:10,  1.68s/it]\u001b[A\n",
      " 83%|███████████████████████████████████▌       | 24/29 [00:38<00:08,  1.68s/it]\u001b[A\n",
      " 86%|█████████████████████████████████████      | 25/29 [00:40<00:06,  1.68s/it]\u001b[A\n",
      " 90%|██████████████████████████████████████▌    | 26/29 [00:41<00:05,  1.68s/it]\u001b[A\n",
      " 93%|████████████████████████████████████████   | 27/29 [00:43<00:03,  1.68s/it]\u001b[A\n",
      " 97%|█████████████████████████████████████████▌ | 28/29 [00:45<00:01,  1.68s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.0791401863098145, 'eval_runtime': 48.6621, 'eval_samples_per_second': 2.384, 'eval_steps_per_second': 0.596, 'epoch': 5.0}\n",
      "100%|█████████████████████████████████████| 1165/1165 [1:47:14<00:00,  5.31s/it]\n",
      "100%|███████████████████████████████████████████| 29/29 [00:47<00:00,  1.68s/it]\u001b[A\n",
      "{'train_runtime': 6434.9262, 'train_samples_per_second': 0.724, 'train_steps_per_second': 0.181, 'train_loss': 1.1493741182810246, 'epoch': 5.0}\n",
      "100%|█████████████████████████████████████| 1165/1165 [1:47:14<00:00,  5.31s/it]Loading best peft model from llama-test/checkpoint-1165 (score: 1.0791401863098145).\n",
      "100%|█████████████████████████████████████| 1165/1165 [1:47:14<00:00,  5.52s/it]\n",
      "> \u001b[1mINFO    Finished training, saving model...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!autotrain llm --train \\\n",
    "              --project_name \"llama-test\" \\\n",
    "              --data_path data/llama_test \\\n",
    "              --train_split train \\\n",
    "              --valid_split val \\\n",
    "              --text_column text \\\n",
    "              --model TinyPixel/Llama-2-7B-bf16-sharded \\\n",
    "              --learning_rate 3e-5 \\\n",
    "              --num_train_epochs 5 \\\n",
    "              --train_batch_size 4 \\\n",
    "              --use_peft \\\n",
    "              --use_int4 \\\n",
    "              --lora_r 16 \\\n",
    "              --lora_alpha 32 \\\n",
    "              --lora_dropout 0.05 \\\n",
    "#               --use_flash_attention_2 \\\n",
    "              --trainer sft \\\n",
    "              --model_max_length 512 \\\n",
    "              --block_size 512 > training.log\n",
    "\n",
    "# --push_to_hub \\\n",
    "# --repo_id sarx11/llama-test \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## User CLI Input \n",
    "# !autotrain llm --inference \\\n",
    "#               --project_name \"llama-test_0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3jQnmK_gqzQC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j1OBInGHXx1o"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h2ENaqRRX73P"
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "02m91ztWX7ED"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/envs/owl-botu/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9DsdSYwTYILr"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaTokenizerFast(name_or_path='./llama-test', vocab_size=32000, model_max_length=1024, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '</s>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checkpoint = \"TinyPixel/Llama-2-7B-bf16-sharded\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "# model = AutoModelForCausalLM.from_pretrained(checkpoint)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./llama-test\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "31hUSExMY4__"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 14/14 [01:04<00:00,  4.60s/it]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"./llama-test/\") # Loads checkpoint shards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "id": "P4ypoDbHd8rk",
    "outputId": "9d162b80-81ff-409a-978a-4f1ffcf1c406",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>aggression</th>\n",
       "      <th>offense</th>\n",
       "      <th>codemixed</th>\n",
       "      <th>tweet_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7624</td>\n",
       "      <td>192426</td>\n",
       "      <td>1.585850e+18</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>@user Congress is not a political party.. It i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5663</td>\n",
       "      <td>159896</td>\n",
       "      <td>1.581150e+18</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>@user लगता है सरकार कोई है ही नही...इसपे UAPA ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0      id      tweet_id  aggression  offense  codemixed  \\\n",
       "0        7624  192426  1.585850e+18           2        1          0   \n",
       "1        5663  159896  1.581150e+18           2        1          1   \n",
       "\n",
       "                                          tweet_text  \n",
       "0  @user Congress is not a political party.. It i...  \n",
       "1  @user लगता है सरकार कोई है ही नही...इसपे UAPA ...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/splits/val.csv\") # data/llama_test/val.csv\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "2beQZ5PXn-fD"
   },
   "outputs": [],
   "source": [
    "system_prompt = \"You are an expert in hate speech detection. Offensive tweets are defined as tweets containing profane words, sarcastic remarks, insults, slanders or slurs. These can have a potentially harmful effect on a given target. Classify the following input tweet as Offensive or Non-Offensive.\"\n",
    "label_map = {1: \"Offensive\", 0: \"Non-Offensive\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_prompt_val(row):\n",
    "    prompt = system_prompt + \"\\n\\n### Input: \" + row[\"tweet_text\"] + \"\\n\\n### Response: \"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['You are an expert in hate speech detection. Offensive tweets are defined as tweets containing profane words, sarcastic remarks, insults, slanders or slurs. These can have a potentially harmful effect on a given target. Classify the following input tweet as Offensive or Non-Offensive.\\n\\n### Input: @user Congress is not a political party.. It is a INC Pvt. Ltd. made by royal Gandhi family for loot people and build new scams.. @user @user @user \\n\\n@user @user\\n\\n### Response: ',\n",
       "       'You are an expert in hate speech detection. Offensive tweets are defined as tweets containing profane words, sarcastic remarks, insults, slanders or slurs. These can have a potentially harmful effect on a given target. Classify the following input tweet as Offensive or Non-Offensive.\\n\\n### Input: @user लगता है सरकार कोई है ही नही...इसपे UAPA लगना चाहिए और साथ ही इसके घर पे बुलडोझर चलना चाहिए..\\n\\n### Response: '],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"text\"] = df.apply(lambda row: prepare_prompt_val(row), axis=1)\n",
    "df[\"text\"].values[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on 1 sample\n",
    "https://huggingface.co/docs/transformers/main/model_doc/llama#transformers.LlamaForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "          )\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "          )\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 158])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(df[\"text\"][2], padding=True, truncation=True, max_length=512, return_tensors=\"pt\").to(device)\n",
    "inputs.input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['You are an expert in hate speech detection. Offensive tweets are defined as tweets containing profane words, sarcastic remarks, insults, slanders or slurs. These can have a potentially harmful effect on a given target. Classify the following input tweet as Offensive or Non-Offensive.\\n\\n### Input: Two Sadhus beaten in Chhatishgarh  suspecting they are childlifters. Its a sad thing that Hindu seers are beaten up this way under false charges the moment they try to stop conversion.\\nSwami Laxmananand Saraswati was killed in Odisha because he got Ghar wapasi done in thousands\\n\\n### Response:  Non-Offensive You are an expert in hate speech detection. Offensive tweets are defined as tweets containing profane words, sarcastic remarks, insults, slanders or slurs. These can have a potentially harmful effect on a given target. Classify the following input tweet as Offensive or Non-Offensive.\\n\\n### Input: @user @user @user @user @user @user @user @user @user @user @user @user @user @user @user @user @user @user @user @user @user @user @user @user @user @user @user @user @user @user @user @']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    generate_ids = model.generate(inputs.input_ids, max_length=300)\n",
    "    \n",
    "tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on batched samples\n",
    "https://huggingface.co/docs/transformers/main/model_doc/llama#transformers.LlamaForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 188])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(df[\"text\"][:4].tolist(), padding=True, truncation=True, max_length=512, return_tensors=\"pt\").to(device)\n",
    "inputs.input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['You are an expert in hate speech detection. Offensive tweets are defined as tweets containing profane words, sarcastic remarks, insults, slanders or slurs. These can have a potentially harmful effect on a given target. Classify the following input tweet as Offensive or Non-Offensive.\\n\\n### Input: @user Congress is not a political party.. It is a INC Pvt. Ltd. made by royal Gandhi family for loot people and build new scams.. @user @user @user \\n\\n@user @user\\n\\n### Response: ',\n",
       " 'You are an expert in hate speech detection. Offensive tweets are defined as tweets containing profane words, sarcastic remarks, insults, slanders or slurs. These can have a potentially harmful effect on a given target. Classify the following input tweet as Offensive or Non-Offensive.\\n\\n### Input: @user लगता है सरकार कोई है ही नही...इसपे UAPA लगना चाहिए और साथ ही इसके घर पे बुलडोझर चलना चाहिए..\\n\\n### Response:  Off',\n",
       " 'You are an expert in hate speech detection. Offensive tweets are defined as tweets containing profane words, sarcastic remarks, insults, slanders or slurs. These can have a potentially harmful effect on a given target. Classify the following input tweet as Offensive or Non-Offensive.\\n\\n### Input: Two Sadhus beaten in Chhatishgarh  suspecting they are childlifters. Its a sad thing that Hindu seers are beaten up this way under false charges the moment they try to stop conversion.\\nSwami Laxmananand Saraswati was killed in Odisha because he got Ghar wapasi done in thousands\\n\\n### Response: \\n',\n",
       " \"You are an expert in hate speech detection. Offensive tweets are defined as tweets containing profane words, sarcastic remarks, insults, slanders or slurs. These can have a potentially harmful effect on a given target. Classify the following input tweet as Offensive or Non-Offensive.\\n\\n### Input: @user @user No they are not. By Muslim they mean brown.\\nIt's like when MAGAs refer to all central ad south Americans as Mexicans.\\nJust shear ignorance.\\n\\n### Response: \\n\"]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    generate_ids = model.generate(inputs.input_ids, max_length=100)\n",
    "    \n",
    "responses = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "responses\n",
    "# [response.split(\"### Response: \")[1] for response in responses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Val Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HSDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inputs = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        labels = torch.tensor(self.labels[idx])\n",
    "        return inputs, labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_tweets = tokenizer(df['text'].tolist(), padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "tokenized_tweets = tokenized_tweets.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = HSDataset(tokenized_tweets, df['offense'].tolist())\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "1it [00:01,  1.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 0, 1, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "2it [00:03,  1.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 1, 0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:05,  2.57s/it]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "responses = []\n",
    "\n",
    "\n",
    "for i, (inputs, labels) in tqdm(enumerate(val_dataloader)):\n",
    "    with torch.no_grad():\n",
    "        generate_ids = model.generate(inputs[\"input_ids\"], max_length=300)\n",
    "    \n",
    "    response = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, \n",
    "                                      clean_up_tokenization_spaces=False)\n",
    "    responses = responses + response\n",
    "    #if i == 2:\n",
    "        #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['You are an expert in hate speech detection. Offensive tweets are defined as tweets containing profane words, sarcastic remarks, insults, slanders or slurs. These can have a potentially harmful effect on a given target. Classify the following input tweet as Offensive or Non-Offensive.\\n\\n### Input: @user Congress is not a political party.. It is a INC Pvt. Ltd. made by royal Gandhi family for loot people and build new scams.. @user @user @user \\n\\n@user @user\\n\\n### Response: ',\n",
       " 'You are an expert in hate speech detection. Offensive tweets are defined as tweets containing profane words, sarcastic remarks, insults, slanders or slurs. These can have a potentially harmful effect on a given target. Classify the following input tweet as Offensive or Non-Offensive.\\n\\n### Input: @user लगता है सरकार कोई है ही नही...इसपे UAPA लगना चाहिए और साथ ही इसके घर पे बुलडोझर चलना चाहिए..\\n\\n### Response: \\n',\n",
       " 'You are an expert in hate speech detection. Offensive tweets are defined as tweets containing profane words, sarcastic remarks, insults, slanders or slurs. These can have a potentially harmful effect on a given target. Classify the following input tweet as Offensive or Non-Offensive.\\n\\n### Input: Two Sadhus beaten in Chhatishgarh  suspecting they are childlifters. Its a sad thing that Hindu seers are beaten up this way under false charges the moment they try to stop conversion.\\nSwami Laxmananand Saraswati was killed in Odisha because he got Ghar wapasi done in thousands\\n\\n### Response: ',\n",
       " \"You are an expert in hate speech detection. Offensive tweets are defined as tweets containing profane words, sarcastic remarks, insults, slanders or slurs. These can have a potentially harmful effect on a given target. Classify the following input tweet as Offensive or Non-Offensive.\\n\\n### Input: @user @user No they are not. By Muslim they mean brown.\\nIt's like when MAGAs refer to all central ad south Americans as Mexicans.\\nJust shear ignorance.\\n\\n### Response: \",\n",
       " 'You are an expert in hate speech detection. Offensive tweets are defined as tweets containing profane words, sarcastic remarks, insults, slanders or slurs. These can have a potentially harmful effect on a given target. Classify the following input tweet as Offensive or Non-Offensive.\\n\\n### Input: @user Look is stupid\\n\\n### Response: ',\n",
       " 'You are an expert in hate speech detection. Offensive tweets are defined as tweets containing profane words, sarcastic remarks, insults, slanders or slurs. These can have a potentially harmful effect on a given target. Classify the following input tweet as Offensive or Non-Offensive.\\n\\n### Input: @user And all that you said is correct but still doesn\\'t answer my question that if they have clearly objectified it is a contract where the man has paid money can now enjoy all that, how can now she put a marital rape case on him when she technically  \"sold\" herself\\n\\n### Response: ',\n",
       " 'You are an expert in hate speech detection. Offensive tweets are defined as tweets containing profane words, sarcastic remarks, insults, slanders or slurs. These can have a potentially harmful effect on a given target. Classify the following input tweet as Offensive or Non-Offensive.\\n\\n### Input: @user Usko jail ho, na ho. Par usne jin logo ko convert kiya, unki ghar wapasi hogi ki nahi? Hogi to tab hogi? Ye important hai.\\n\\n### Response: ',\n",
       " 'You are an expert in hate speech detection. Offensive tweets are defined as tweets containing profane words, sarcastic remarks, insults, slanders or slurs. These can have a potentially harmful effect on a given target. Classify the following input tweet as Offensive or Non-Offensive.\\n\\n### Input: @user Designed by CITG/MSA VSSC/ISRO  🥲 🥲\\n\\n### Response: ',\n",
       " 'You are an expert in hate speech detection. Offensive tweets are defined as tweets containing profane words, sarcastic remarks, insults, slanders or slurs. These can have a potentially harmful effect on a given target. Classify the following input tweet as Offensive or Non-Offensive.\\n\\n### Input: Tribals are fighting to regain the \"Adivasi\" identity forcibly snatched away by the rulers for the past several decades.\\nF\\n#आदिवासी_जनगणना_कॉलम_दो\\n\\n### Response: ',\n",
       " 'You are an expert in hate speech detection. Offensive tweets are defined as tweets containing profane words, sarcastic remarks, insults, slanders or slurs. These can have a potentially harmful effect on a given target. Classify the following input tweet as Offensive or Non-Offensive.\\n\\n### Input: While talking about the outcome of the Quit India Movement Golwalkar said: Sangh vowed not to do anything directly  #RSSQuitIndia\\n\\n### Response: ',\n",
       " 'You are an expert in hate speech detection. Offensive tweets are defined as tweets containing profane words, sarcastic remarks, insults, slanders or slurs. These can have a potentially harmful effect on a given target. Classify the following input tweet as Offensive or Non-Offensive.\\n\\n### Input: @user It was not a foolish mistake.\\nIt was done purposely. \\nNehru being a Muslim wanted to help Pakistan.\\nHe and his daughter did everything to see that India did not progress.\\n\\n### Response: ',\n",
       " 'You are an expert in hate speech detection. Offensive tweets are defined as tweets containing profane words, sarcastic remarks, insults, slanders or slurs. These can have a potentially harmful effect on a given target. Classify the following input tweet as Offensive or Non-Offensive.\\n\\n### Input: @user Lakshmi ganesh+shreeram gandhi ka nahi\\n\\n### Response: ']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96, 153.4906103286385, 228)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lens = df['text'].apply(lambda x: len(x.split())) * 2\n",
    "lens.min(), lens.mean(), lens.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 85/852 [09:57<1:34:20,  7.38s/it]/home/sn3250/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 256, but `max_length` is set to 256. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "100%|██████████| 852/852 [1:40:08<00:00,  7.05s/it]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "responses = []\n",
    "\n",
    "for i in tqdm(range(len(df))):\n",
    "    inputs = tokenizer(df[\"text\"][i], padding=True, truncation=True, max_length=256, \n",
    "                       return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        generate_ids = model.generate(inputs.input_ids, max_length=256)\n",
    "        \n",
    "    response = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, \n",
    "                                      clean_up_tokenization_spaces=False)[0]\n",
    "    responses.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "852"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(responses):\n",
    "    labels = []\n",
    "    response_trimmed = 0\n",
    "    label_absent = 0\n",
    "\n",
    "    for response in responses:\n",
    "        splitted = response.split(\"### Response: \")\n",
    "        if len(splitted) == 1:\n",
    "            #print(response, \"\\n\")\n",
    "            response_trimmed += 1\n",
    "            label = 0 #-1\n",
    "            \n",
    "        else:\n",
    "            if \"Non-Offensive\" in splitted[1][:15]:\n",
    "                label = 0\n",
    "            elif \"Offensive\" in splitted[1][:15]:\n",
    "                label = 1\n",
    "            else:\n",
    "                label_absent += 1\n",
    "                label = 0 # Default majority class\n",
    "                \n",
    "        labels.append(label)\n",
    "\n",
    "    print(f\"{response_trimmed} responses trimmed due to max_length\")\n",
    "    print(f\"{label_absent} labels absent \\n\")\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 738, 1: 114})\n",
      "offense\n",
      "0    596\n",
      "1    256\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(Counter(labels))\n",
    "print(df[\"offense\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3837837837837838"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(df['offense'].tolist(), labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   precision    recall  f1-score   support\n",
      "\n",
      "Non-Offensive (0)       0.75      0.93      0.83       596\n",
      "    Offensive (1)       0.62      0.28      0.38       256\n",
      "\n",
      "         accuracy                           0.73       852\n",
      "        macro avg       0.69      0.60      0.61       852\n",
      "     weighted avg       0.71      0.73      0.70       852\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(df['offense'].tolist(), labels, target_names=[\"Non-Offensive (0)\", \"Offensive (1)\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "id": "P4ypoDbHd8rk",
    "outputId": "9d162b80-81ff-409a-978a-4f1ffcf1c406",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>aggression</th>\n",
       "      <th>offense</th>\n",
       "      <th>codemixed</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7683</td>\n",
       "      <td>192521</td>\n",
       "      <td>1.585840e+18</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>@user Can we send your beloved hero Pappu Gand...</td>\n",
       "      <td>You are an expert in hate speech detection. Of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7121</td>\n",
       "      <td>181769</td>\n",
       "      <td>1.162640e+18</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>@user @user @user #HumanRights priorities are ...</td>\n",
       "      <td>You are an expert in hate speech detection. Of...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0      id      tweet_id  aggression  offense  codemixed  \\\n",
       "0        7683  192521  1.585840e+18           2        1          1   \n",
       "1        7121  181769  1.162640e+18           1        0          1   \n",
       "\n",
       "                                          tweet_text  \\\n",
       "0  @user Can we send your beloved hero Pappu Gand...   \n",
       "1  @user @user @user #HumanRights priorities are ...   \n",
       "\n",
       "                                                text  \n",
       "0  You are an expert in hate speech detection. Of...  \n",
       "1  You are an expert in hate speech detection. Of...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv(\"data/splits/test.csv\")\n",
    "test_df[\"text\"] = test_df.apply(lambda row: prepare_prompt_val(row), axis=1)\n",
    "test_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 50/851 [08:29<2:04:30,  9.33s/it]/home/sn3250/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 300, but `max_length` is set to 300. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "100%|██████████| 851/851 [2:16:27<00:00,  9.62s/it]  \n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "responses = []\n",
    "\n",
    "for i in tqdm(range(len(test_df))):\n",
    "    inputs = tokenizer(test_df[\"text\"][i], padding=True, truncation=True, max_length=300, \n",
    "                       return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        generate_ids = model.generate(inputs.input_ids, max_length=300)\n",
    "        \n",
    "    response = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, \n",
    "                                      clean_up_tokenization_spaces=False)[0]\n",
    "    responses.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 responses trimmed due to max_length\n",
      "3 labels absent \n",
      "\n"
     ]
    }
   ],
   "source": [
    "labels = get_labels(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/predictions/llama-ft-clm_test.pickle', 'wb') as f:\n",
    "    pickle.dump(labels, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 744, 1: 107})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4132231404958678"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(test_df['offense'].tolist(), labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   precision    recall  f1-score   support\n",
      "\n",
      "Non-Offensive (0)     0.7567    0.9462    0.8409       595\n",
      "    Offensive (1)     0.7009    0.2930    0.4132       256\n",
      "\n",
      "         accuracy                         0.7497       851\n",
      "        macro avg     0.7288    0.6196    0.6271       851\n",
      "     weighted avg     0.7399    0.7497    0.7123       851\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_df['offense'].tolist(), labels, \n",
    "                            target_names=[\"Non-Offensive (0)\", \"Offensive (1)\"], digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0198773448773446"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum = 0\n",
    "for i in range(1,12):\n",
    "    sum += 1/i\n",
    "sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03333333333333333"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.1/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "owl-botu",
   "language": "python",
   "name": "owl-botu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "138a00acb8ab483eaf3fd2f4950c331e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "168ed95c80ec47788d5d1c5c9d185cc5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1db46b14f00046268671521b2d012b56": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "CheckboxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "CheckboxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "CheckboxView",
      "description": "Add token as git credential?",
      "description_tooltip": null,
      "disabled": false,
      "indent": true,
      "layout": "IPY_MODEL_d4500e5f6a544ce48d60c787afa7ca6d",
      "style": "IPY_MODEL_90ac34c25725489289a78bb79ca521aa",
      "value": true
     }
    },
    "2b416b118ce94f4cb7f513619cb87bf4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ButtonView",
      "button_style": "",
      "description": "Login",
      "disabled": false,
      "icon": "",
      "layout": "IPY_MODEL_3c71a42665714532b47f73912c895100",
      "style": "IPY_MODEL_72cba92167214e87856d58282dd845c9",
      "tooltip": ""
     }
    },
    "2e219f9a04f648b5a3f342847fb9561b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_89e575c7547a4a38a90c236d674adf17",
      "placeholder": "​",
      "style": "IPY_MODEL_cdf2f83962e24bc1add8eddffb3e3e75",
      "value": "Your token has been saved to /root/.cache/huggingface/token"
     }
    },
    "38b402b65e1743a286ac55cd2f087bdf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3c57b6a02c3446d486a32a32303e991e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f68a25ba21d84f148a3be62bbf50fae6",
       "IPY_MODEL_e9bfbfb0a1fd46ba9d805f1f5ff4f035",
       "IPY_MODEL_2e219f9a04f648b5a3f342847fb9561b",
       "IPY_MODEL_d709ded56fa64151a0ed26aa72902b3a"
      ],
      "layout": "IPY_MODEL_f885a8337a954d84b85adb069532ae7c"
     }
    },
    "3c71a42665714532b47f73912c895100": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "519578f5613d44b588197d10be6aeca4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5d9850fa815a4b9490f4ead789eb4b34": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6a1b835820b64589b61488dbebec97f8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "72cba92167214e87856d58282dd845c9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "button_color": null,
      "font_weight": ""
     }
    },
    "89e575c7547a4a38a90c236d674adf17": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "90ac34c25725489289a78bb79ca521aa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "922834622dcc4cab9533818980228020": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d1f307aa6a1c48c398b0eb00357e7a27",
      "placeholder": "​",
      "style": "IPY_MODEL_a968489cdd094b6ab9ca29e05f7a78ce",
      "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
     }
    },
    "9499ca3b334b4d59bb90e0655e00b12e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a968489cdd094b6ab9ca29e05f7a78ce": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c0ad09d29a8548799e2f902b542a9b92": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f2618f77277f445e891acad8ecf4332c",
      "placeholder": "​",
      "style": "IPY_MODEL_6a1b835820b64589b61488dbebec97f8",
      "value": "Connecting..."
     }
    },
    "cdf2f83962e24bc1add8eddffb3e3e75": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d1f307aa6a1c48c398b0eb00357e7a27": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d4500e5f6a544ce48d60c787afa7ca6d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d709ded56fa64151a0ed26aa72902b3a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_38b402b65e1743a286ac55cd2f087bdf",
      "placeholder": "​",
      "style": "IPY_MODEL_5d9850fa815a4b9490f4ead789eb4b34",
      "value": "Login successful"
     }
    },
    "e9bfbfb0a1fd46ba9d805f1f5ff4f035": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_168ed95c80ec47788d5d1c5c9d185cc5",
      "placeholder": "​",
      "style": "IPY_MODEL_138a00acb8ab483eaf3fd2f4950c331e",
      "value": "Your token has been saved in your configured git credential helpers (store)."
     }
    },
    "eba67e43e28d46689c2dd9cc5a0b777a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_519578f5613d44b588197d10be6aeca4",
      "placeholder": "​",
      "style": "IPY_MODEL_9499ca3b334b4d59bb90e0655e00b12e",
      "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
     }
    },
    "ed4bd6243d6b477fae1f3e5363e0b530": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "PasswordModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "PasswordModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "PasswordView",
      "continuous_update": true,
      "description": "Token:",
      "description_tooltip": null,
      "disabled": false,
      "layout": "IPY_MODEL_f531674720ef4f31a8e923397417fd53",
      "placeholder": "​",
      "style": "IPY_MODEL_fccd73cc865a410ab275828458d5ca99",
      "value": ""
     }
    },
    "f096a21d30674e9d8b5ecc31d1c3df89": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f2618f77277f445e891acad8ecf4332c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f531674720ef4f31a8e923397417fd53": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f68a25ba21d84f148a3be62bbf50fae6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f096a21d30674e9d8b5ecc31d1c3df89",
      "placeholder": "​",
      "style": "IPY_MODEL_fe7e5e2baf11494286e8aeeb917226bb",
      "value": "Token is valid (permission: read)."
     }
    },
    "f885a8337a954d84b85adb069532ae7c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": "center",
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "flex",
      "flex": null,
      "flex_flow": "column",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "50%"
     }
    },
    "fccd73cc865a410ab275828458d5ca99": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fe7e5e2baf11494286e8aeeb917226bb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
