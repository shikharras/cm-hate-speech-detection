{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/envs/owl-botu/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "import torch\n",
    "#from torch.utils.data import Dataset\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, pipeline\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data & Create Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prompt Example**:\n",
    "\n",
    "You are an expert in hate speech detection. Offensive tweets are defined as tweets containing profane words, sarcastic remarks, insults, slanders or slurs. These can have a potentially harmful effect on a given target. Classify the following input tweet as Offensive or Non-Offensive.\n",
    "\n",
    "`###` Input: <tweet>\n",
    "\n",
    "`###` Response: Offensive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are an expert in hate speech detection. Offensive tweets are defined as tweets containing profane words, sarcastic remarks, insults, slanders or slurs. These can have a potentially harmful effect on a given target. Classify the following input tweet as Offensive or Non-Offensive.\"\n",
    "label_map = {1: \"Offensive\", 0: \"Non-Offensive\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_prompt(row, train=True):\n",
    "    # Data Format -- https://huggingface.co/datasets/vicgalle/alpaca-gpt4?row=0\n",
    "    prompt = system_prompt + \"\\n\\n ### Input: \" + row[\"tweet_text\"] + \"\\n\\n ### Response: \"\n",
    "    if train:\n",
    "         prompt = prompt + label_map[row[\"offense\"]] # Add label\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Unnamed: 0', 'id', 'tweet_id', 'aggression', 'offense', 'codemixed', 'tweet_text', 'text'],\n",
       "    num_rows: 6809\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"data/splits/train.csv\")\n",
    "val_df = pd.read_csv(\"data/splits/val.csv\")\n",
    "\n",
    "# Prompt Column\n",
    "train_df[\"text\"] = train_df.apply(lambda row: prepare_prompt(row), axis=1)\n",
    "val_df[\"text\"] = val_df.apply(lambda row: prepare_prompt(row), axis=1)\n",
    "\n",
    "# HF Datasets\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Quantized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"TinyPixel/Llama-2-7B-bf16-sharded\"\n",
    "# checkpoint = \"/vast/work/public/ml-datasets/llama-2/Llama-2-7b-hf/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4-bit Quantization (config passed to AutoModel.from_pretrained)\n",
    "quant_config = BitsAndBytesConfig(\n",
    "                            load_in_4bit = True,\n",
    "                            bnb_4bit_quant_type = \"nf4\", # Normal Float 4-bits\n",
    "                            bnb_4bit_compute_dtype = getattr(torch, \"float16\"), # torch.float16\n",
    "                            bnb_4bit_use_double_quant = False,\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 14/14 [01:04<00:00,  4.63s/it]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(checkpoint, quantization_config=quant_config)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "tokenizer.pad_token = tokenizer.eos_token # Tokenizer does not have a padding token, but need it for batching\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# model.config.use_cache = False\n",
    "# model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"Trainable params = {trainable_params} | All params = {all_param} | Trainable % = {100 * trainable_params / all_param:.2f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params = 262410240 | All params = 3500412928 | Trainable % = 7.50\n"
     ]
    }
   ],
   "source": [
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Training Args & PEFT LoRA adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter Efficient Fine-Tuning with LoRA adapters. (config passed to SFT Trainer)\n",
    "peft_config = LoraConfig(\n",
    "                    lora_alpha=16,\n",
    "                    lora_dropout=0.1,\n",
    "                    r=64,\n",
    "                    bias=\"none\",\n",
    "                    task_type=\"CAUSAL_LM\",\n",
    "                    )\n",
    "# Note: If doing classification, need to wrap model in peft class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_params = TrainingArguments(\n",
    "                            output_dir=\"./results\",\n",
    "                            num_train_epochs=3,\n",
    "                            per_device_train_batch_size=4,\n",
    "                            gradient_accumulation_steps=1,\n",
    "                            optim=\"paged_adamw_32bit\",\n",
    "                            save_steps=25,\n",
    "                            logging_steps=25,\n",
    "                            learning_rate=5e-5,\n",
    "                            weight_decay=0.001,\n",
    "                            fp16=False,\n",
    "                            bf16=False,\n",
    "                            max_grad_norm=0.3,\n",
    "                            max_steps=-1,\n",
    "                            warmup_ratio=0.03,\n",
    "                            group_by_length=True,\n",
    "                            evaluation_strategy=\"epoch\",\n",
    "                            save_strategy=\"epoch\",\n",
    "                            load_best_model_at_end=True,\n",
    "                            lr_scheduler_type=\"constant\",\n",
    "                            report_to=\"tensorboard\"\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with SFT Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Response Template for CompletionOnlyLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataCollatorForCompletionOnlyLM: Only train the model to generate the response (instead of the full sequence)\n",
    "https://huggingface.co/docs/trl/sft_trainer#advanced-usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_template = \"### Response:\"\n",
    "collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 6809/6809 [00:03<00:00, 1968.11 examples/s]\n",
      "Map: 100%|██████████| 852/852 [00:00<00:00, 5749.09 examples/s]\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "                model = model,\n",
    "                train_dataset = train_dataset,\n",
    "                eval_dataset = val_dataset,\n",
    "                data_collator = collator,\n",
    "                peft_config = peft_config,                \n",
    "                dataset_text_field = \"text\",\n",
    "                max_seq_length = 512,\n",
    "                tokenizer = tokenizer,\n",
    "                args = training_params,\n",
    "                packing = False, # To not pack examples to fill seq len\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Unnamed: 0': 6587,\n",
       " 'id': 169269,\n",
       " 'tweet_id': 1.58569e+18,\n",
       " 'aggression': 0,\n",
       " 'offense': 0,\n",
       " 'codemixed': 1,\n",
       " 'tweet_text': \"Let's get some zimbabwe players into ipl and invite them to play series against india in india #zimbabwe #PAKvsZIM #T20worldcup22\",\n",
       " 'text': \"You are an expert in hate speech detection. Offensive tweets are defined as tweets containing profane words, sarcastic remarks, insults, slanders or slurs. These can have a potentially harmful effect on a given target. Classify the following input tweet as Offensive or Non-Offensive.\\n\\n ### Input: Let's get some zimbabwe players into ipl and invite them to play series against india in india #zimbabwe #PAKvsZIM #T20worldcup22\\n\\n ### Response: Non-Offensive\"}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using 8-bit optimizers with a version of `bitsandbytes` < 0.41.1. It is recommended to update your version as a major bug has been fixed in 8-bit optimizers.\n",
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/ext3/miniconda3/envs/owl-botu/lib/python3.10/site-packages/trl/trainer/utils.py:120: UserWarning: Could not find response key `### Response:` in the following instance: <s> You are an expert in hate speech detection. Offensive tweets are defined as tweets containing profane words, sarcastic remarks, insults, slanders or slurs. These can have a potentially harmful effect on a given target. Classify the following input tweet as Offensive or Non-Offensive.\n",
      "\n",
      " ### Input: RSS ಬಹಿರಂಗವಾಗಿ ತಲ್ವಾರ್ ,ತ್ರಿಶೂಲ ಮೆರವಣಿಗೆ ಮಾಡಿದಾಗ,ಆಯುದ ಪೂಜೆಯ ಹೆಸರಲ್ಲಿ ಅಕ್ರಮ ಶಸ್ತ್ರಾಸ್ತ್ರ ಬಂದೂಕುಗಳಿಗೆ ಪೂಜೆ ಮಾಡಿದಾಗ ಇಲ್ಲದ UAPA ,ಯಾವುದೇ ಸೂಕ್ತ ಪುರಾವೆ ಇಲ್ಲದಿದ್ದರೂ ಸಂಶಯದ ಆಧಾರದ ಮೇಲೆ SDPI ನಾಯಕರನ್ನು ������� This instance will be ignored in loss calculation. Note, if this happens often, consider increasing the `max_seq_length`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5109' max='5109' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5109/5109 2:42:48, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.094500</td>\n",
       "      <td>0.126849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.115300</td>\n",
       "      <td>0.110914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.080500</td>\n",
       "      <td>0.144403</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/envs/owl-botu/lib/python3.10/site-packages/trl/trainer/utils.py:120: UserWarning: Could not find response key `### Response:` in the following instance: <s> You are an expert in hate speech detection. Offensive tweets are defined as tweets containing profane words, sarcastic remarks, insults, slanders or slurs. These can have a potentially harmful effect on a given target. Classify the following input tweet as Offensive or Non-Offensive.\n",
      "\n",
      " ### Input: @user @user కానీ గురువులు ఇలాంటి వాటికి అతీతులు అని ఈ వ్యాసంలో బాగా చెప్పారు, \n",
      "RSS Golwalkar ji కూడా ఒకసారి  పీఠాధిపతులు అందరిని సమావేశపరిచారు. ఈ వివరాలు చాలా తక్కువ తెలుసు బయటకి.\n",
      "\n",
      "Very nice reading..\n",
      "వ్యక్తి కన్ This instance will be ignored in loss calculation. Note, if this happens often, consider increasing the `max_seq_length`.\n",
      "  warnings.warn(\n",
      "/ext3/miniconda3/envs/owl-botu/lib/python3.10/site-packages/trl/trainer/utils.py:120: UserWarning: Could not find response key `### Response:` in the following instance: <s> You are an expert in hate speech detection. Offensive tweets are defined as tweets containing profane words, sarcastic remarks, insults, slanders or slurs. These can have a potentially harmful effect on a given target. Classify the following input tweet as Offensive or Non-Offensive.\n",
      "\n",
      " ### Input: @user @user కానీ గురువులు ఇలాంటి వాటికి అతీతులు అని ఈ వ్యాసంలో బాగా చెప్పారు, \n",
      "RSS Golwalkar ji కూడా ఒకసారి  పీఠాధిపతులు అందరిని సమావేశపరిచారు. ఈ వివరాలు చాలా తక్కువ తెలుసు బయటకి.\n",
      "\n",
      "Very nice reading..\n",
      "వ్యక్తి కన్ This instance will be ignored in loss calculation. Note, if this happens often, consider increasing the `max_seq_length`.\n",
      "  warnings.warn(\n",
      "/ext3/miniconda3/envs/owl-botu/lib/python3.10/site-packages/trl/trainer/utils.py:120: UserWarning: Could not find response key `### Response:` in the following instance: <s> You are an expert in hate speech detection. Offensive tweets are defined as tweets containing profane words, sarcastic remarks, insults, slanders or slurs. These can have a potentially harmful effect on a given target. Classify the following input tweet as Offensive or Non-Offensive.\n",
      "\n",
      " ### Input: RSS ಬಹಿರಂಗವಾಗಿ ತಲ್ವಾರ್ ,ತ್ರಿಶೂಲ ಮೆರವಣಿಗೆ ಮಾಡಿದಾಗ,ಆಯುದ ಪೂಜೆಯ ಹೆಸರಲ್ಲಿ ಅಕ್ರಮ ಶಸ್ತ್ರಾಸ್ತ್ರ ಬಂದೂಕುಗಳಿಗೆ ಪೂಜೆ ಮಾಡಿದಾಗ ಇಲ್ಲದ UAPA ,ಯಾವುದೇ ಸೂಕ್ತ ಪುರಾವೆ ಇಲ್ಲದಿದ್ದರೂ ಸಂಶಯದ ಆಧಾರದ ಮೇಲೆ SDPI ನಾಯಕರನ್ನು ������� This instance will be ignored in loss calculation. Note, if this happens often, consider increasing the `max_seq_length`.\n",
      "  warnings.warn(\n",
      "/ext3/miniconda3/envs/owl-botu/lib/python3.10/site-packages/trl/trainer/utils.py:120: UserWarning: Could not find response key `### Response:` in the following instance: <s> You are an expert in hate speech detection. Offensive tweets are defined as tweets containing profane words, sarcastic remarks, insults, slanders or slurs. These can have a potentially harmful effect on a given target. Classify the following input tweet as Offensive or Non-Offensive.\n",
      "\n",
      " ### Input: RSS ಬಹಿರಂಗವಾಗಿ ತಲ್ವಾರ್ ,ತ್ರಿಶೂಲ ಮೆರವಣಿಗೆ ಮಾಡಿದಾಗ,ಆಯುದ ಪೂಜೆಯ ಹೆಸರಲ್ಲಿ ಅಕ್ರಮ ಶಸ್ತ್ರಾಸ್ತ್ರ ಬಂದೂಕುಗಳಿಗೆ ಪೂಜೆ ಮಾಡಿದಾಗ ಇಲ್ಲದ UAPA ,ಯಾವುದೇ ಸೂಕ್ತ ಪುರಾವೆ ಇಲ್ಲದಿದ್ದರೂ ಸಂಶಯದ ಆಧಾರದ ಮೇಲೆ SDPI ನಾಯಕರನ್ನು ������� This instance will be ignored in loss calculation. Note, if this happens often, consider increasing the `max_seq_length`.\n",
      "  warnings.warn(\n",
      "/ext3/miniconda3/envs/owl-botu/lib/python3.10/site-packages/trl/trainer/utils.py:120: UserWarning: Could not find response key `### Response:` in the following instance: <s> You are an expert in hate speech detection. Offensive tweets are defined as tweets containing profane words, sarcastic remarks, insults, slanders or slurs. These can have a potentially harmful effect on a given target. Classify the following input tweet as Offensive or Non-Offensive.\n",
      "\n",
      " ### Input: @user @user కానీ గురువులు ఇలాంటి వాటికి అతీతులు అని ఈ వ్యాసంలో బాగా చెప్పారు, \n",
      "RSS Golwalkar ji కూడా ఒకసారి  పీఠాధిపతులు అందరిని సమావేశపరిచారు. ఈ వివరాలు చాలా తక్కువ తెలుసు బయటకి.\n",
      "\n",
      "Very nice reading..\n",
      "వ్యక్తి కన్ This instance will be ignored in loss calculation. Note, if this happens often, consider increasing the `max_seq_length`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5109, training_loss=0.128015772380295, metrics={'train_runtime': 9781.6529, 'train_samples_per_second': 2.088, 'train_steps_per_second': 0.522, 'total_flos': 1.1505941595367834e+17, 'train_loss': 0.128015772380295, 'epoch': 3.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'loss': 0.3425, 'learning_rate': 5e-05, 'epoch': 0.01, 'step': 25},\n",
       " {'loss': 0.1533, 'learning_rate': 5e-05, 'epoch': 0.03, 'step': 50},\n",
       " {'loss': 0.2537, 'learning_rate': 5e-05, 'epoch': 0.04, 'step': 75},\n",
       " {'loss': 0.1619, 'learning_rate': 5e-05, 'epoch': 0.06, 'step': 100},\n",
       " {'loss': 0.1774, 'learning_rate': 5e-05, 'epoch': 0.07, 'step': 125},\n",
       " {'loss': 0.1743, 'learning_rate': 5e-05, 'epoch': 0.09, 'step': 150},\n",
       " {'loss': 0.2213, 'learning_rate': 5e-05, 'epoch': 0.1, 'step': 175},\n",
       " {'loss': 0.1322, 'learning_rate': 5e-05, 'epoch': 0.12, 'step': 200},\n",
       " {'loss': 0.1909, 'learning_rate': 5e-05, 'epoch': 0.13, 'step': 225},\n",
       " {'loss': 0.2162, 'learning_rate': 5e-05, 'epoch': 0.15, 'step': 250},\n",
       " {'loss': 0.145, 'learning_rate': 5e-05, 'epoch': 0.16, 'step': 275},\n",
       " {'loss': 0.1548, 'learning_rate': 5e-05, 'epoch': 0.18, 'step': 300},\n",
       " {'loss': 0.1923, 'learning_rate': 5e-05, 'epoch': 0.19, 'step': 325},\n",
       " {'loss': 0.1337, 'learning_rate': 5e-05, 'epoch': 0.21, 'step': 350},\n",
       " {'loss': 0.214, 'learning_rate': 5e-05, 'epoch': 0.22, 'step': 375},\n",
       " {'loss': 0.1516, 'learning_rate': 5e-05, 'epoch': 0.23, 'step': 400},\n",
       " {'loss': 0.1772, 'learning_rate': 5e-05, 'epoch': 0.25, 'step': 425},\n",
       " {'loss': 0.1392, 'learning_rate': 5e-05, 'epoch': 0.26, 'step': 450},\n",
       " {'loss': 0.1791, 'learning_rate': 5e-05, 'epoch': 0.28, 'step': 475},\n",
       " {'loss': 0.1388, 'learning_rate': 5e-05, 'epoch': 0.29, 'step': 500},\n",
       " {'loss': 0.1364, 'learning_rate': 5e-05, 'epoch': 0.31, 'step': 525},\n",
       " {'loss': 0.1481, 'learning_rate': 5e-05, 'epoch': 0.32, 'step': 550},\n",
       " {'loss': 0.1615, 'learning_rate': 5e-05, 'epoch': 0.34, 'step': 575},\n",
       " {'loss': 0.144, 'learning_rate': 5e-05, 'epoch': 0.35, 'step': 600},\n",
       " {'loss': 0.1633, 'learning_rate': 5e-05, 'epoch': 0.37, 'step': 625},\n",
       " {'loss': 0.1444, 'learning_rate': 5e-05, 'epoch': 0.38, 'step': 650},\n",
       " {'loss': 0.1578, 'learning_rate': 5e-05, 'epoch': 0.4, 'step': 675},\n",
       " {'loss': 0.1795, 'learning_rate': 5e-05, 'epoch': 0.41, 'step': 700},\n",
       " {'loss': 0.162, 'learning_rate': 5e-05, 'epoch': 0.43, 'step': 725},\n",
       " {'loss': 0.1554, 'learning_rate': 5e-05, 'epoch': 0.44, 'step': 750},\n",
       " {'loss': 0.1902, 'learning_rate': 5e-05, 'epoch': 0.46, 'step': 775},\n",
       " {'loss': 0.1211, 'learning_rate': 5e-05, 'epoch': 0.47, 'step': 800},\n",
       " {'loss': 0.1436, 'learning_rate': 5e-05, 'epoch': 0.48, 'step': 825},\n",
       " {'loss': 0.139, 'learning_rate': 5e-05, 'epoch': 0.5, 'step': 850},\n",
       " {'loss': 0.1426, 'learning_rate': 5e-05, 'epoch': 0.51, 'step': 875},\n",
       " {'loss': 0.1448, 'learning_rate': 5e-05, 'epoch': 0.53, 'step': 900},\n",
       " {'loss': 0.1363, 'learning_rate': 5e-05, 'epoch': 0.54, 'step': 925},\n",
       " {'loss': 0.148, 'learning_rate': 5e-05, 'epoch': 0.56, 'step': 950},\n",
       " {'loss': 0.1652, 'learning_rate': 5e-05, 'epoch': 0.57, 'step': 975},\n",
       " {'loss': 0.1161, 'learning_rate': 5e-05, 'epoch': 0.59, 'step': 1000},\n",
       " {'loss': 0.1627, 'learning_rate': 5e-05, 'epoch': 0.6, 'step': 1025},\n",
       " {'loss': 0.1097, 'learning_rate': 5e-05, 'epoch': 0.62, 'step': 1050},\n",
       " {'loss': 0.1777, 'learning_rate': 5e-05, 'epoch': 0.63, 'step': 1075},\n",
       " {'loss': 0.2338, 'learning_rate': 5e-05, 'epoch': 0.65, 'step': 1100},\n",
       " {'loss': 0.1479, 'learning_rate': 5e-05, 'epoch': 0.66, 'step': 1125},\n",
       " {'loss': 0.1179, 'learning_rate': 5e-05, 'epoch': 0.68, 'step': 1150},\n",
       " {'loss': 0.1792, 'learning_rate': 5e-05, 'epoch': 0.69, 'step': 1175},\n",
       " {'loss': 0.1468, 'learning_rate': 5e-05, 'epoch': 0.7, 'step': 1200},\n",
       " {'loss': 0.1243, 'learning_rate': 5e-05, 'epoch': 0.72, 'step': 1225},\n",
       " {'loss': 0.148, 'learning_rate': 5e-05, 'epoch': 0.73, 'step': 1250},\n",
       " {'loss': 0.1799, 'learning_rate': 5e-05, 'epoch': 0.75, 'step': 1275},\n",
       " {'loss': 0.1315, 'learning_rate': 5e-05, 'epoch': 0.76, 'step': 1300},\n",
       " {'loss': 0.1684, 'learning_rate': 5e-05, 'epoch': 0.78, 'step': 1325},\n",
       " {'loss': 0.1511, 'learning_rate': 5e-05, 'epoch': 0.79, 'step': 1350},\n",
       " {'loss': 0.1219, 'learning_rate': 5e-05, 'epoch': 0.81, 'step': 1375},\n",
       " {'loss': 0.1302, 'learning_rate': 5e-05, 'epoch': 0.82, 'step': 1400},\n",
       " {'loss': 0.1299, 'learning_rate': 5e-05, 'epoch': 0.84, 'step': 1425},\n",
       " {'loss': 0.1708, 'learning_rate': 5e-05, 'epoch': 0.85, 'step': 1450},\n",
       " {'loss': 0.1744, 'learning_rate': 5e-05, 'epoch': 0.87, 'step': 1475},\n",
       " {'loss': 0.1432, 'learning_rate': 5e-05, 'epoch': 0.88, 'step': 1500},\n",
       " {'loss': 0.1231, 'learning_rate': 5e-05, 'epoch': 0.9, 'step': 1525},\n",
       " {'loss': 0.1028, 'learning_rate': 5e-05, 'epoch': 0.91, 'step': 1550},\n",
       " {'loss': 0.1334, 'learning_rate': 5e-05, 'epoch': 0.92, 'step': 1575},\n",
       " {'loss': 0.1267, 'learning_rate': 5e-05, 'epoch': 0.94, 'step': 1600},\n",
       " {'loss': 0.1525, 'learning_rate': 5e-05, 'epoch': 0.95, 'step': 1625},\n",
       " {'loss': 0.1328, 'learning_rate': 5e-05, 'epoch': 0.97, 'step': 1650},\n",
       " {'loss': 0.1456, 'learning_rate': 5e-05, 'epoch': 0.98, 'step': 1675},\n",
       " {'loss': 0.0945, 'learning_rate': 5e-05, 'epoch': 1.0, 'step': 1700},\n",
       " {'eval_loss': 0.12684927880764008,\n",
       "  'eval_runtime': 220.1049,\n",
       "  'eval_samples_per_second': 3.871,\n",
       "  'eval_steps_per_second': 0.486,\n",
       "  'epoch': 1.0,\n",
       "  'step': 1703},\n",
       " {'loss': 0.1183, 'learning_rate': 5e-05, 'epoch': 1.01, 'step': 1725},\n",
       " {'loss': 0.1569, 'learning_rate': 5e-05, 'epoch': 1.03, 'step': 1750},\n",
       " {'loss': 0.1314, 'learning_rate': 5e-05, 'epoch': 1.04, 'step': 1775},\n",
       " {'loss': 0.1282, 'learning_rate': 5e-05, 'epoch': 1.06, 'step': 1800},\n",
       " {'loss': 0.0958, 'learning_rate': 5e-05, 'epoch': 1.07, 'step': 1825},\n",
       " {'loss': 0.107, 'learning_rate': 5e-05, 'epoch': 1.09, 'step': 1850},\n",
       " {'loss': 0.1223, 'learning_rate': 5e-05, 'epoch': 1.1, 'step': 1875},\n",
       " {'loss': 0.1294, 'learning_rate': 5e-05, 'epoch': 1.12, 'step': 1900},\n",
       " {'loss': 0.1453, 'learning_rate': 5e-05, 'epoch': 1.13, 'step': 1925},\n",
       " {'loss': 0.12, 'learning_rate': 5e-05, 'epoch': 1.15, 'step': 1950},\n",
       " {'loss': 0.0906, 'learning_rate': 5e-05, 'epoch': 1.16, 'step': 1975},\n",
       " {'loss': 0.1303, 'learning_rate': 5e-05, 'epoch': 1.17, 'step': 2000},\n",
       " {'loss': 0.1199, 'learning_rate': 5e-05, 'epoch': 1.19, 'step': 2025},\n",
       " {'loss': 0.1168, 'learning_rate': 5e-05, 'epoch': 1.2, 'step': 2050},\n",
       " {'loss': 0.1757, 'learning_rate': 5e-05, 'epoch': 1.22, 'step': 2075},\n",
       " {'loss': 0.1042, 'learning_rate': 5e-05, 'epoch': 1.23, 'step': 2100},\n",
       " {'loss': 0.161, 'learning_rate': 5e-05, 'epoch': 1.25, 'step': 2125},\n",
       " {'loss': 0.1124, 'learning_rate': 5e-05, 'epoch': 1.26, 'step': 2150},\n",
       " {'loss': 0.1511, 'learning_rate': 5e-05, 'epoch': 1.28, 'step': 2175},\n",
       " {'loss': 0.1038, 'learning_rate': 5e-05, 'epoch': 1.29, 'step': 2200},\n",
       " {'loss': 0.1756, 'learning_rate': 5e-05, 'epoch': 1.31, 'step': 2225},\n",
       " {'loss': 0.1586, 'learning_rate': 5e-05, 'epoch': 1.32, 'step': 2250},\n",
       " {'loss': 0.1467, 'learning_rate': 5e-05, 'epoch': 1.34, 'step': 2275},\n",
       " {'loss': 0.1311, 'learning_rate': 5e-05, 'epoch': 1.35, 'step': 2300},\n",
       " {'loss': 0.1536, 'learning_rate': 5e-05, 'epoch': 1.37, 'step': 2325},\n",
       " {'loss': 0.1192, 'learning_rate': 5e-05, 'epoch': 1.38, 'step': 2350},\n",
       " {'loss': 0.1347, 'learning_rate': 5e-05, 'epoch': 1.39, 'step': 2375},\n",
       " {'loss': 0.11, 'learning_rate': 5e-05, 'epoch': 1.41, 'step': 2400},\n",
       " {'loss': 0.1217, 'learning_rate': 5e-05, 'epoch': 1.42, 'step': 2425},\n",
       " {'loss': 0.1329, 'learning_rate': 5e-05, 'epoch': 1.44, 'step': 2450},\n",
       " {'loss': 0.1122, 'learning_rate': 5e-05, 'epoch': 1.45, 'step': 2475},\n",
       " {'loss': 0.1015, 'learning_rate': 5e-05, 'epoch': 1.47, 'step': 2500},\n",
       " {'loss': 0.0924, 'learning_rate': 5e-05, 'epoch': 1.48, 'step': 2525},\n",
       " {'loss': 0.1005, 'learning_rate': 5e-05, 'epoch': 1.5, 'step': 2550},\n",
       " {'loss': 0.1704, 'learning_rate': 5e-05, 'epoch': 1.51, 'step': 2575},\n",
       " {'loss': 0.1139, 'learning_rate': 5e-05, 'epoch': 1.53, 'step': 2600},\n",
       " {'loss': 0.1176, 'learning_rate': 5e-05, 'epoch': 1.54, 'step': 2625},\n",
       " {'loss': 0.0631, 'learning_rate': 5e-05, 'epoch': 1.56, 'step': 2650},\n",
       " {'loss': 0.1624, 'learning_rate': 5e-05, 'epoch': 1.57, 'step': 2675},\n",
       " {'loss': 0.1227, 'learning_rate': 5e-05, 'epoch': 1.59, 'step': 2700},\n",
       " {'loss': 0.1476, 'learning_rate': 5e-05, 'epoch': 1.6, 'step': 2725},\n",
       " {'loss': 0.1277, 'learning_rate': 5e-05, 'epoch': 1.61, 'step': 2750},\n",
       " {'loss': 0.1137, 'learning_rate': 5e-05, 'epoch': 1.63, 'step': 2775},\n",
       " {'loss': 0.1521, 'learning_rate': 5e-05, 'epoch': 1.64, 'step': 2800},\n",
       " {'loss': 0.1323, 'learning_rate': 5e-05, 'epoch': 1.66, 'step': 2825},\n",
       " {'loss': 0.097, 'learning_rate': 5e-05, 'epoch': 1.67, 'step': 2850},\n",
       " {'loss': 0.1675, 'learning_rate': 5e-05, 'epoch': 1.69, 'step': 2875},\n",
       " {'loss': 0.1025, 'learning_rate': 5e-05, 'epoch': 1.7, 'step': 2900},\n",
       " {'loss': 0.1151, 'learning_rate': 5e-05, 'epoch': 1.72, 'step': 2925},\n",
       " {'loss': 0.0908, 'learning_rate': 5e-05, 'epoch': 1.73, 'step': 2950},\n",
       " {'loss': 0.1021, 'learning_rate': 5e-05, 'epoch': 1.75, 'step': 2975},\n",
       " {'loss': 0.0928, 'learning_rate': 5e-05, 'epoch': 1.76, 'step': 3000},\n",
       " {'loss': 0.0898, 'learning_rate': 5e-05, 'epoch': 1.78, 'step': 3025},\n",
       " {'loss': 0.105, 'learning_rate': 5e-05, 'epoch': 1.79, 'step': 3050},\n",
       " {'loss': 0.127, 'learning_rate': 5e-05, 'epoch': 1.81, 'step': 3075},\n",
       " {'loss': 0.1038, 'learning_rate': 5e-05, 'epoch': 1.82, 'step': 3100},\n",
       " {'loss': 0.1553, 'learning_rate': 5e-05, 'epoch': 1.83, 'step': 3125},\n",
       " {'loss': 0.1122, 'learning_rate': 5e-05, 'epoch': 1.85, 'step': 3150},\n",
       " {'loss': 0.1061, 'learning_rate': 5e-05, 'epoch': 1.86, 'step': 3175},\n",
       " {'loss': 0.1211, 'learning_rate': 5e-05, 'epoch': 1.88, 'step': 3200},\n",
       " {'loss': 0.1455, 'learning_rate': 5e-05, 'epoch': 1.89, 'step': 3225},\n",
       " {'loss': 0.1172, 'learning_rate': 5e-05, 'epoch': 1.91, 'step': 3250},\n",
       " {'loss': 0.1203, 'learning_rate': 5e-05, 'epoch': 1.92, 'step': 3275},\n",
       " {'loss': 0.1201, 'learning_rate': 5e-05, 'epoch': 1.94, 'step': 3300},\n",
       " {'loss': 0.1022, 'learning_rate': 5e-05, 'epoch': 1.95, 'step': 3325},\n",
       " {'loss': 0.0955, 'learning_rate': 5e-05, 'epoch': 1.97, 'step': 3350},\n",
       " {'loss': 0.1273, 'learning_rate': 5e-05, 'epoch': 1.98, 'step': 3375},\n",
       " {'loss': 0.1153, 'learning_rate': 5e-05, 'epoch': 2.0, 'step': 3400},\n",
       " {'eval_loss': 0.1109139695763588,\n",
       "  'eval_runtime': 218.4047,\n",
       "  'eval_samples_per_second': 3.901,\n",
       "  'eval_steps_per_second': 0.49,\n",
       "  'epoch': 2.0,\n",
       "  'step': 3406},\n",
       " {'loss': 0.0852, 'learning_rate': 5e-05, 'epoch': 2.01, 'step': 3425},\n",
       " {'loss': 0.1545, 'learning_rate': 5e-05, 'epoch': 2.03, 'step': 3450},\n",
       " {'loss': 0.096, 'learning_rate': 5e-05, 'epoch': 2.04, 'step': 3475},\n",
       " {'loss': 0.1067, 'learning_rate': 5e-05, 'epoch': 2.06, 'step': 3500},\n",
       " {'loss': 0.0836, 'learning_rate': 5e-05, 'epoch': 2.07, 'step': 3525},\n",
       " {'loss': 0.1094, 'learning_rate': 5e-05, 'epoch': 2.08, 'step': 3550},\n",
       " {'loss': 0.0677, 'learning_rate': 5e-05, 'epoch': 2.1, 'step': 3575},\n",
       " {'loss': 0.0881, 'learning_rate': 5e-05, 'epoch': 2.11, 'step': 3600},\n",
       " {'loss': 0.0706, 'learning_rate': 5e-05, 'epoch': 2.13, 'step': 3625},\n",
       " {'loss': 0.0651, 'learning_rate': 5e-05, 'epoch': 2.14, 'step': 3650},\n",
       " {'loss': 0.1565, 'learning_rate': 5e-05, 'epoch': 2.16, 'step': 3675},\n",
       " {'loss': 0.1051, 'learning_rate': 5e-05, 'epoch': 2.17, 'step': 3700},\n",
       " {'loss': 0.1297, 'learning_rate': 5e-05, 'epoch': 2.19, 'step': 3725},\n",
       " {'loss': 0.0989, 'learning_rate': 5e-05, 'epoch': 2.2, 'step': 3750},\n",
       " {'loss': 0.1109, 'learning_rate': 5e-05, 'epoch': 2.22, 'step': 3775},\n",
       " {'loss': 0.0566, 'learning_rate': 5e-05, 'epoch': 2.23, 'step': 3800},\n",
       " {'loss': 0.1215, 'learning_rate': 5e-05, 'epoch': 2.25, 'step': 3825},\n",
       " {'loss': 0.092, 'learning_rate': 5e-05, 'epoch': 2.26, 'step': 3850},\n",
       " {'loss': 0.1142, 'learning_rate': 5e-05, 'epoch': 2.28, 'step': 3875},\n",
       " {'loss': 0.0843, 'learning_rate': 5e-05, 'epoch': 2.29, 'step': 3900},\n",
       " {'loss': 0.0576, 'learning_rate': 5e-05, 'epoch': 2.3, 'step': 3925},\n",
       " {'loss': 0.1143, 'learning_rate': 5e-05, 'epoch': 2.32, 'step': 3950},\n",
       " {'loss': 0.1371, 'learning_rate': 5e-05, 'epoch': 2.33, 'step': 3975},\n",
       " {'loss': 0.091, 'learning_rate': 5e-05, 'epoch': 2.35, 'step': 4000},\n",
       " {'loss': 0.1063, 'learning_rate': 5e-05, 'epoch': 2.36, 'step': 4025},\n",
       " {'loss': 0.0589, 'learning_rate': 5e-05, 'epoch': 2.38, 'step': 4050},\n",
       " {'loss': 0.1344, 'learning_rate': 5e-05, 'epoch': 2.39, 'step': 4075},\n",
       " {'loss': 0.1359, 'learning_rate': 5e-05, 'epoch': 2.41, 'step': 4100},\n",
       " {'loss': 0.1542, 'learning_rate': 5e-05, 'epoch': 2.42, 'step': 4125},\n",
       " {'loss': 0.1152, 'learning_rate': 5e-05, 'epoch': 2.44, 'step': 4150},\n",
       " {'loss': 0.1322, 'learning_rate': 5e-05, 'epoch': 2.45, 'step': 4175},\n",
       " {'loss': 0.1026, 'learning_rate': 5e-05, 'epoch': 2.47, 'step': 4200},\n",
       " {'loss': 0.0853, 'learning_rate': 5e-05, 'epoch': 2.48, 'step': 4225},\n",
       " {'loss': 0.0499, 'learning_rate': 5e-05, 'epoch': 2.5, 'step': 4250},\n",
       " {'loss': 0.1091, 'learning_rate': 5e-05, 'epoch': 2.51, 'step': 4275},\n",
       " {'loss': 0.0812, 'learning_rate': 5e-05, 'epoch': 2.52, 'step': 4300},\n",
       " {'loss': 0.1023, 'learning_rate': 5e-05, 'epoch': 2.54, 'step': 4325},\n",
       " {'loss': 0.1143, 'learning_rate': 5e-05, 'epoch': 2.55, 'step': 4350},\n",
       " {'loss': 0.1159, 'learning_rate': 5e-05, 'epoch': 2.57, 'step': 4375},\n",
       " {'loss': 0.1045, 'learning_rate': 5e-05, 'epoch': 2.58, 'step': 4400},\n",
       " {'loss': 0.0648, 'learning_rate': 5e-05, 'epoch': 2.6, 'step': 4425},\n",
       " {'loss': 0.1368, 'learning_rate': 5e-05, 'epoch': 2.61, 'step': 4450},\n",
       " {'loss': 0.1347, 'learning_rate': 5e-05, 'epoch': 2.63, 'step': 4475},\n",
       " {'loss': 0.1231, 'learning_rate': 5e-05, 'epoch': 2.64, 'step': 4500},\n",
       " {'loss': 0.1169, 'learning_rate': 5e-05, 'epoch': 2.66, 'step': 4525},\n",
       " {'loss': 0.091, 'learning_rate': 5e-05, 'epoch': 2.67, 'step': 4550},\n",
       " {'loss': 0.0818, 'learning_rate': 5e-05, 'epoch': 2.69, 'step': 4575},\n",
       " {'loss': 0.1333, 'learning_rate': 5e-05, 'epoch': 2.7, 'step': 4600},\n",
       " {'loss': 0.0993, 'learning_rate': 5e-05, 'epoch': 2.72, 'step': 4625},\n",
       " {'loss': 0.084, 'learning_rate': 5e-05, 'epoch': 2.73, 'step': 4650},\n",
       " {'loss': 0.1542, 'learning_rate': 5e-05, 'epoch': 2.75, 'step': 4675},\n",
       " {'loss': 0.0807, 'learning_rate': 5e-05, 'epoch': 2.76, 'step': 4700},\n",
       " {'loss': 0.1271, 'learning_rate': 5e-05, 'epoch': 2.77, 'step': 4725},\n",
       " {'loss': 0.0956, 'learning_rate': 5e-05, 'epoch': 2.79, 'step': 4750},\n",
       " {'loss': 0.1081, 'learning_rate': 5e-05, 'epoch': 2.8, 'step': 4775},\n",
       " {'loss': 0.0807, 'learning_rate': 5e-05, 'epoch': 2.82, 'step': 4800},\n",
       " {'loss': 0.1177, 'learning_rate': 5e-05, 'epoch': 2.83, 'step': 4825},\n",
       " {'loss': 0.084, 'learning_rate': 5e-05, 'epoch': 2.85, 'step': 4850},\n",
       " {'loss': 0.1318, 'learning_rate': 5e-05, 'epoch': 2.86, 'step': 4875},\n",
       " {'loss': 0.1076, 'learning_rate': 5e-05, 'epoch': 2.88, 'step': 4900},\n",
       " {'loss': 0.0989, 'learning_rate': 5e-05, 'epoch': 2.89, 'step': 4925},\n",
       " {'loss': 0.1092, 'learning_rate': 5e-05, 'epoch': 2.91, 'step': 4950},\n",
       " {'loss': 0.1244, 'learning_rate': 5e-05, 'epoch': 2.92, 'step': 4975},\n",
       " {'loss': 0.0843, 'learning_rate': 5e-05, 'epoch': 2.94, 'step': 5000},\n",
       " {'loss': 0.1348, 'learning_rate': 5e-05, 'epoch': 2.95, 'step': 5025},\n",
       " {'loss': 0.0671, 'learning_rate': 5e-05, 'epoch': 2.97, 'step': 5050},\n",
       " {'loss': 0.108, 'learning_rate': 5e-05, 'epoch': 2.98, 'step': 5075},\n",
       " {'loss': 0.0805, 'learning_rate': 5e-05, 'epoch': 2.99, 'step': 5100},\n",
       " {'eval_loss': 0.14440281689167023,\n",
       "  'eval_runtime': 217.1037,\n",
       "  'eval_samples_per_second': 3.924,\n",
       "  'eval_steps_per_second': 0.493,\n",
       "  'epoch': 3.0,\n",
       "  'step': 5109},\n",
       " {'train_runtime': 9781.6529,\n",
       "  'train_samples_per_second': 2.088,\n",
       "  'train_steps_per_second': 0.522,\n",
       "  'total_flos': 1.1505941595367834e+17,\n",
       "  'train_loss': 0.128015772380295,\n",
       "  'epoch': 3.0,\n",
       "  'step': 5109}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.state.log_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=64, out_features=4096, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "          )\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(\n",
       "            in_features=4096, out_features=4096, bias=False\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=64, out_features=4096, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "          )\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_checkpoint = \"./results/checkpoint-5109/\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(trained_checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(trained_checkpoint)\n",
    "model = model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = pd.read_csv(\"data/splits/val.csv\")\n",
    "test_df = pd.read_csv(\"data/splits/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['You are an expert in hate speech detection. Offensive tweets are defined as tweets containing profane words, sarcastic remarks, insults, slanders or slurs. These can have a potentially harmful effect on a given target. Classify the following input tweet as Offensive or Non-Offensive.\\n\\n ### Input: @user Congress is not a political party.. It is a INC Pvt. Ltd. made by royal Gandhi family for loot people and build new scams.. @user @user @user \\n\\n@user @user\\n\\n ### Response: ',\n",
       "       'You are an expert in hate speech detection. Offensive tweets are defined as tweets containing profane words, sarcastic remarks, insults, slanders or slurs. These can have a potentially harmful effect on a given target. Classify the following input tweet as Offensive or Non-Offensive.\\n\\n ### Input: @user लगता है सरकार कोई है ही नही...इसपे UAPA लगना चाहिए और साथ ही इसके घर पे बुलडोझर चलना चाहिए..\\n\\n ### Response: '],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df[\"text\"] = val_df.apply(lambda row: prepare_prompt(row, train=False), axis=1)\n",
    "test_df[\"text\"] = test_df.apply(lambda row: prepare_prompt(row, train=False), axis=1)\n",
    "\n",
    "val_df[\"text\"].values[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['You are an expert in hate speech detection. Offensive tweets are defined as tweets containing profane words, sarcastic remarks, insults, slanders or slurs. These can have a potentially harmful effect on a given target. Classify the following input tweet as Offensive or Non-Offensive.\\n\\n ### Input: @user Congress is not a political party.. It is a INC Pvt. Ltd. made by royal Gandhi family for loot people and build new scams.. @user @user @user \\n\\n@user @user\\n\\n ### Response:  Offensive\\n\\n### Input: @user @user @user @user @user @user @user']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(val_df[\"text\"][0], padding=True, truncation=True, max_length=512, return_tensors=\"pt\").to(device)\n",
    "with torch.no_grad():\n",
    "    generate_ids = model.generate(inputs.input_ids, max_length=150)\n",
    "    \n",
    "tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://github.com/huggingface/transformers/issues/23017\n",
    "* https://discuss.huggingface.co/t/results-of-model-generate-are-different-for-different-batch-sizes-of-the-decode-only-model/34878"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['You are an expert in hate speech detection. Offensive tweets are defined as tweets containing profane words, sarcastic remarks, insults, slanders or slurs. These can have a potentially harmful effect on a given target. Classify the following input tweet as Offensive or Non-Offensive.\\n\\n ### Input: @user Congress is not a political party.. It is a INC Pvt. Ltd. made by royal Gandhi family for loot people and build new scams.. @user @user @user \\n\\n@user @user\\n\\n ### Response: .\\nMS.\\n\\n\\n.\\n\\n.\\n\\n\\n',\n",
       " 'You are an expert in hate speech detection. Offensive tweets are defined as tweets containing profane words, sarcastic remarks, insults, slanders or slurs. These can have a potentially harmful effect on a given target. Classify the following input tweet as Offensive or Non-Offensive.\\n\\n ### Input: @user लगता है सरकार कोई है ही नही...इसपे UAPA लगना चाहिए और साथ ही इसके घर पे बुलडोझर चलना चाहिए..\\n\\n ### Response:  Offensive\\n\\n ### Input: @user क्या',\n",
       " 'You are an expert in hate speech detection. Offensive tweets are defined as tweets containing profane words, sarcastic remarks, insults, slanders or slurs. These can have a potentially harmful effect on a given target. Classify the following input tweet as Offensive or Non-Offensive.\\n\\n ### Input: Two Sadhus beaten in Chhatishgarh  suspecting they are childlifters. Its a sad thing that Hindu seers are beaten up this way under false charges the moment they try to stop conversion.\\nSwami Laxmananand Saraswati was killed in Odisha because he got Ghar wapasi done in thousands\\n\\n ### Response: 1.1.1.1.1.1.1.',\n",
       " \"You are an expert in hate speech detection. Offensive tweets are defined as tweets containing profane words, sarcastic remarks, insults, slanders or slurs. These can have a potentially harmful effect on a given target. Classify the following input tweet as Offensive or Non-Offensive.\\n\\n ### Input: @user @user No they are not. By Muslim they mean brown.\\nIt's like when MAGAs refer to all central ad south Americans as Mexicans.\\nJust shear ignorance.\\n\\n ### Response: . and.\\nMS...\\n\\n\\n..\\n\"]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(val_df[\"text\"][:4].tolist(), padding=True, truncation=True, max_length=512, return_tensors=\"pt\").to(device)\n",
    "inputs.input_ids.shape\n",
    "\n",
    "with torch.no_grad():\n",
    "    generate_ids = model.generate(inputs.input_ids, max_length=200)\n",
    "    \n",
    "responses = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "responses\n",
    "# [response.split(\"### Response: \")[1] for response in responses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_responses(df):\n",
    "    model.eval()\n",
    "    responses = []\n",
    "\n",
    "    for i in tqdm(range(len(df))):\n",
    "        inputs = tokenizer(df[\"text\"][i], padding=True, truncation=True, max_length=256, \n",
    "                           return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            generate_ids = model.generate(inputs.input_ids, max_length=256)\n",
    "\n",
    "        response = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, \n",
    "                                          clean_up_tokenization_spaces=False)[0]\n",
    "        responses.append(response)\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(responses):\n",
    "    labels = []\n",
    "    response_trimmed = 0\n",
    "    label_absent = 0\n",
    "\n",
    "    for response in responses:\n",
    "        splitted = response.split(\"### Response: \")\n",
    "        if len(splitted) == 1:\n",
    "            #print(response, \"\\n\")\n",
    "            response_trimmed += 1\n",
    "            label = 0 #-1\n",
    "            \n",
    "        else:\n",
    "            if \"Non-Offensive\" in splitted[1][:15]:\n",
    "                label = 0\n",
    "            elif \"Offensive\" in splitted[1][:15]:\n",
    "                label = 1\n",
    "            else:\n",
    "                label_absent += 1\n",
    "                label = 0 # Default majority class\n",
    "                \n",
    "        labels.append(label)\n",
    "\n",
    "    print(f\"{response_trimmed} responses trimmed due to max_length\")\n",
    "    print(f\"{label_absent} labels absent \\n\")\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(labels, df):\n",
    "    print(\"F1 score = \", f1_score(df['offense'].tolist(), labels))\n",
    "    print(classification_report(df['offense'].tolist(), labels, target_names=[\"Non-Offensive (0)\", \"Offensive (1)\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 85/852 [10:08<1:36:10,  7.52s/it]/home/sn3250/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 256, but `max_length` is set to 256. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "100%|██████████| 852/852 [1:42:08<00:00,  7.19s/it]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "responses = []\n",
    "\n",
    "for i in tqdm(range(len(val_df))):\n",
    "    inputs = tokenizer(val_df[\"text\"][i], padding=True, truncation=True, max_length=256, \n",
    "                       return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        generate_ids = model.generate(inputs.input_ids, max_length=256)\n",
    "        \n",
    "    response = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, \n",
    "                                      clean_up_tokenization_spaces=False)[0]\n",
    "    responses.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_responses = inference_responses(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_responses = responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 responses trimmed due to max_length\n",
      "17 labels absent \n",
      "\n",
      "F1 score =  0.6997840172786178\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "Non-Offensive (0)       0.85      0.92      0.89       596\n",
      "    Offensive (1)       0.78      0.63      0.70       256\n",
      "\n",
      "         accuracy                           0.84       852\n",
      "        macro avg       0.82      0.78      0.79       852\n",
      "     weighted avg       0.83      0.84      0.83       852\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val_labels = get_labels(val_responses)\n",
    "print_metrics(val_labels, val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/predictions/llama-ft-completion_val.pickle', 'wb') as f:\n",
    "    pickle.dump(val_labels, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# with open('data/predictions/llama-ft-completion_val.pickle', 'rb') as handle:\n",
    "#     val_labels = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 50/851 [06:18<1:32:01,  6.89s/it]/home/sn3250/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 256, but `max_length` is set to 256. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "100%|██████████| 851/851 [1:42:53<00:00,  7.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 responses trimmed due to max_length\n",
      "22 labels absent \n",
      "\n",
      "F1 score =  0.6696230598669624\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "Non-Offensive (0)       0.84      0.93      0.88       595\n",
      "    Offensive (1)       0.77      0.59      0.67       256\n",
      "\n",
      "         accuracy                           0.82       851\n",
      "        macro avg       0.81      0.76      0.78       851\n",
      "     weighted avg       0.82      0.82      0.82       851\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_responses = inference_responses(test_df)\n",
    "test_labels = get_labels(test_responses)\n",
    "print_metrics(test_labels, test_df)\n",
    "\n",
    "with open('data/predictions/llama-ft-completion_test.pickle', 'wb') as f:\n",
    "    pickle.dump(test_labels, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "* https://www.datacamp.com/tutorial/fine-tuning-llama-2\n",
    "* https://huggingface.co/docs/trl/sft_trainer\n",
    "* https://huggingface.co/docs/peft/task_guides/image_classification_lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "owl-botu",
   "language": "python",
   "name": "owl-botu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
